{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b44159-4161-4f37-94c6-3f490314c382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-01-20 14:02:35,404 - ===== START: SINGLE-CELL CHUNKED KINEMATIC WAVE PINN =====\n",
      "[INFO] 2025-01-20 14:02:35,405 - Python version : 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]\n",
      "[INFO] 2025-01-20 14:02:35,405 - Platform       : Linux-6.8.0-51-generic-x86_64-with-glibc2.35\n",
      "[INFO] 2025-01-20 14:02:35,406 - PyTorch version: 2.2.2\n",
      "[INFO] 2025-01-20 14:02:35,407 - CUDA available : True\n",
      "[INFO] 2025-01-20 14:02:35,407 - CUDA device(s) : 1 GPU(s).\n",
      "[INFO] 2025-01-20 14:02:35,408 - GPU 0 name     : NVIDIA RTX A6000\n",
      "[INFO] 2025-01-20 14:02:35,741 - nvidia-smi output:\n",
      "Mon Jan 20 14:02:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:61:00.0  On |                  Off |\n",
      "| 30%   37C    P8             22W /  300W |     796MiB /  49140MiB |      5%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1608      G   /usr/lib/xorg/Xorg                             90MiB |\n",
      "|    0   N/A  N/A      1968      G   /usr/bin/gnome-shell                          124MiB |\n",
      "|    0   N/A  N/A      4978      G   ...seed-version=20250119-180455.285000         79MiB |\n",
      "|    0   N/A  N/A      5831      C   .../amit/anaconda3/envs/gpu/bin/python        472MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "[INFO] 2025-01-20 14:02:35,742 - ===== ENV CHECK DONE =====\n",
      "\n",
      "[INFO] 2025-01-20 14:02:35,744 - === START TRAINING PINN ===\n",
      "[INFO] 2025-01-20 14:02:35,745 - Loading training data with fraction=0.1, chunk_size=500000, max_rows=2000000\n",
      "[INFO] 2025-01-20 14:02:35,745 - Loading CSV in chunks from: /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2011_merged.csv\n",
      "[INFO] 2025-01-20 14:02:47,966 - Reached max_rows=2000000; stopping chunk read.\n",
      "[INFO] 2025-01-20 14:02:48,011 - Loaded 2000000 rows from /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2011_merged.csv in 12.27 s.\n",
      "[INFO] 2025-01-20 14:02:48,016 - Loading CSV in chunks from: /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2012_merged.csv\n",
      "[INFO] 2025-01-20 14:02:59,564 - Reached max_rows=2000000; stopping chunk read.\n",
      "[INFO] 2025-01-20 14:02:59,610 - Loaded 2000000 rows from /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2012_merged.csv in 11.59 s.\n",
      "[INFO] 2025-01-20 14:02:59,615 - Loading CSV in chunks from: /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2013_merged.csv\n",
      "[INFO] 2025-01-20 14:03:11,181 - Reached max_rows=2000000; stopping chunk read.\n",
      "[INFO] 2025-01-20 14:03:11,234 - Loaded 2000000 rows from /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2013_merged.csv in 11.62 s.\n",
      "[INFO] 2025-01-20 14:03:11,239 - Loading CSV in chunks from: /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2014_merged.csv\n",
      "[INFO] 2025-01-20 14:03:22,534 - Reached max_rows=2000000; stopping chunk read.\n",
      "[INFO] 2025-01-20 14:03:22,577 - Loaded 2000000 rows from /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2014_merged.csv in 11.34 s.\n",
      "[INFO] 2025-01-20 14:03:22,808 - Combined total rows: 8000000\n",
      "[INFO] 2025-01-20 14:03:23,421 - Using device: cuda\n",
      "[INFO] 2025-01-20 14:03:23,448 - Spatial/time domain: x in [35.10,35.56], t in [0.00,1123.98], rain in [0.00,11.13]\n",
      "[INFO] 2025-01-20 14:03:23,449 - num_epochs=300, lr=0.001, pde_weight=1.0, batch_size=512, collocation_points=1000\n",
      "[INFO] 2025-01-20 14:03:23,450 - wave_speed=1.0\n",
      "[INFO] 2025-01-20 17:02:31,188 - Epoch [50/300] Data Loss: 5.9431e-02, PDE Loss: 1.1025e-01\n",
      "[INFO] 2025-01-20 20:00:33,461 - Epoch [100/300] Data Loss: 2.0369e-01, PDE Loss: 2.1885e+01\n",
      "[INFO] 2025-01-20 22:58:35,099 - Epoch [150/300] Data Loss: 8.3844e-02, PDE Loss: 2.2291e+00\n",
      "[INFO] 2025-01-21 01:56:08,320 - Epoch [200/300] Data Loss: 6.7479e-02, PDE Loss: 5.4573e-02\n",
      "[INFO] 2025-01-21 04:53:07,550 - Epoch [250/300] Data Loss: 6.6494e-02, PDE Loss: 6.5555e-02\n",
      "[INFO] 2025-01-21 07:50:50,734 - Epoch [300/300] Data Loss: 6.5380e-02, PDE Loss: 4.0217e-02\n",
      "[INFO] 2025-01-21 07:50:50,735 - Training completed in 1067.45 minutes.\n",
      "[INFO] 2025-01-21 07:50:50,735 - Loading test data...\n",
      "[INFO] 2025-01-21 07:50:50,735 - Loading CSV in chunks from: /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2015_merged.csv\n",
      "[INFO] 2025-01-21 07:53:22,921 - Loaded 190182960 rows from /media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2015_merged.csv in 152.19 s.\n",
      "[INFO] 2025-01-21 07:53:23,433 - Combined total rows: 190182960\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 45.34 GiB. GPU 0 has a total capacity of 47.42 GiB of which 43.70 GiB is free. Including non-PyTorch memory, this process has 3.43 GiB memory in use. Of the allocated memory 3.09 GiB is allocated by PyTorch, and 23.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 351\u001b[0m\n\u001b[1;32m    348\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500_000\u001b[39m\n\u001b[1;32m    349\u001b[0m max_rows_per_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2_000_000\u001b[39m\n\u001b[0;32m--> 351\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pinn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_csv_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_csv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfraction_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows_per_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwave_speed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# fewer epochs for memory test\u001b[39;49;00m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpde_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollocation_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# note corrected usage here\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    364\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m    367\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkinematic_wave_pinn_chunked.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 323\u001b[0m, in \u001b[0;36mtrain_pinn\u001b[0;34m(train_data_paths, test_data_path, fraction, chunk_size, max_rows, wave_speed, num_epochs, lr, pde_weight, batch_size, collocation_points, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 323\u001b[0m     Q_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     test_mse \u001b[38;5;241m=\u001b[39m loss_fn(Q_test, test_out)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    325\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_mse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 167\u001b[0m, in \u001b[0;36mPINN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# x shape: [batch, 3]\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 45.34 GiB. GPU 0 has a total capacity of 47.42 GiB of which 43.70 GiB is free. Including non-PyTorch memory, this process has 3.43 GiB memory in use. Of the allocated memory 3.09 GiB is allocated by PyTorch, and 23.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Single Jupyter Cell: Chunked CSV Loading + PINN for Kinematic Wave (Corrected)\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "################################################################################\n",
    "# 1) LOGGING & ENVIRONMENT CHECK\n",
    "################################################################################\n",
    "logger = logging.getLogger(\"ChunkedKinematicPINN\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Avoid adding multiple handlers if cell is re-run\n",
    "if not logger.handlers:\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(\"===== START: SINGLE-CELL CHUNKED KINEMATIC WAVE PINN =====\")\n",
    "logger.info(f\"Python version : {sys.version}\")\n",
    "logger.info(f\"Platform       : {platform.platform()}\")\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"CUDA available : {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"CUDA device(s) : {torch.cuda.device_count()} GPU(s).\")\n",
    "    logger.info(f\"GPU 0 name     : {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# (Optional) attempt nvidia-smi\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        logger.info(\"nvidia-smi output:\\n\" + result.stdout)\n",
    "    else:\n",
    "        logger.warning(\"Could not run nvidia-smi (non-zero return).\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(\"nvidia-smi not found. Skipping GPU memory check.\")\n",
    "\n",
    "logger.info(\"===== ENV CHECK DONE =====\\n\")\n",
    "\n",
    "################################################################################\n",
    "# 2) CHUNK-BASED CSV LOADING WITH SAMPLING\n",
    "################################################################################\n",
    "\n",
    "def load_csv_in_chunks(path, chunk_size=500_000, fraction=1.0, max_rows=None):\n",
    "    \"\"\"\n",
    "    Reads a CSV in chunks to avoid loading huge files into memory at once.\n",
    "    Optionally samples a fraction of rows from each chunk, \n",
    "    and stops after 'max_rows' if provided.\n",
    "\n",
    "    Args:\n",
    "      path (str): Path to CSV file.\n",
    "      chunk_size (int): # of rows per chunk read.\n",
    "      fraction (float): fraction of rows to keep (random sample) from each chunk.\n",
    "      max_rows (int): if not None, stop after reading this many rows in total.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: concatenated result of the chunked read.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading CSV in chunks from: {path}\")\n",
    "    start_time = time.time()\n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "\n",
    "    for chunk in pd.read_csv(path, chunksize=chunk_size):\n",
    "        # random sample from this chunk if fraction < 1.0\n",
    "        if fraction < 1.0:\n",
    "            chunk = chunk.sample(frac=fraction)\n",
    "\n",
    "        dfs.append(chunk)\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "        if max_rows is not None and total_rows >= max_rows:\n",
    "            logger.info(f\"Reached max_rows={max_rows}; stopping chunk read.\")\n",
    "            break\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"Loaded {len(df)} rows from {path} in {elapsed:.2f} s.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv_as_dataset(paths, fraction=1.0, chunk_size=500_000, max_rows=None):\n",
    "    \"\"\"\n",
    "    Loads multiple CSV files via chunk-based reading. Combines them into one DataFrame,\n",
    "    then creates (x, t, rainrate) as input and (rg_qms) as output. \n",
    "    Returns torch tensors (in_tensor, out_tensor).\n",
    "\n",
    "    fraction (float): fraction of data rows to keep. For huge files, try <1.0\n",
    "    chunk_size (int): # of rows per chunk in read_csv\n",
    "    max_rows (int): optional total row limit per file (stops reading after this many).\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df_chunk = load_csv_in_chunks(\n",
    "            path=p,\n",
    "            chunk_size=chunk_size,\n",
    "            fraction=fraction,\n",
    "            max_rows=max_rows\n",
    "        )\n",
    "        dfs.append(df_chunk)\n",
    "\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    logger.info(f\"Combined total rows: {len(data)}\")\n",
    "\n",
    "    # Convert \"time\" to numeric scale\n",
    "    # Here we do a naive approach: convert to seconds, then offset by min, etc.\n",
    "    data[\"t_numeric\"] = pd.to_datetime(data[\"time\"]).astype(int) / 1e9\n",
    "    t0 = data[\"t_numeric\"].min()\n",
    "    data[\"t\"] = (data[\"t_numeric\"] - t0) / (60.0 * 60.0 * 24.0)  # days\n",
    "\n",
    "    # Use \"longitude\" as x for demonstration\n",
    "    data[\"x\"] = data[\"longitude\"]\n",
    "\n",
    "    # Create input arrays\n",
    "    inputs = np.stack([\n",
    "        data[\"x\"].values.astype(np.float32),\n",
    "        data[\"t\"].values.astype(np.float32),\n",
    "        data[\"rainrate\"].values.astype(np.float32)\n",
    "    ], axis=1)\n",
    "\n",
    "    # The target is flow \"rg_qms\"\n",
    "    Q = data[\"rg_qms\"].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    # Convert to torch\n",
    "    in_tensor = torch.from_numpy(inputs)\n",
    "    out_tensor = torch.from_numpy(Q)\n",
    "    return in_tensor, out_tensor\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 3) PINN DEFINITION & PDE RESIDUAL (KINEMATIC WAVE)\n",
    "################################################################################\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, output_dim=1, num_layers=4):\n",
    "        \"\"\"\n",
    "        input_dim: (x, t, rainrate)\n",
    "        output_dim: Q (flow)\n",
    "        hidden_dim, num_layers: network size\n",
    "        \"\"\"\n",
    "        super(PINN, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            layers.append(nn.Tanh())  # or ReLU, etc.\n",
    "            in_features = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, 3]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def pde_residual(model, x, t, rain, v=1.0):\n",
    "    \"\"\"\n",
    "    Kinematic wave PDE: dQ/dt + v * dQ/dx = rain\n",
    "    => residual = dQ/dt + v*dQ/dx - rain\n",
    "\n",
    "    Returns a tensor of shape [batch_size, 1].\n",
    "    \"\"\"\n",
    "    inp = torch.stack([x, t, rain], dim=1)\n",
    "    inp.requires_grad_()\n",
    "\n",
    "    Q = model(inp)  # [batch_size, 1]\n",
    "\n",
    "    # partial derivatives wrt x, t, (and ignoring rain derivative)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=Q,\n",
    "        inputs=inp,\n",
    "        grad_outputs=torch.ones_like(Q),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]  # shape: [batch_size, 3]\n",
    "\n",
    "    Q_x = grads[:, 0].unsqueeze(-1)  # derivative wrt x\n",
    "    Q_t = grads[:, 1].unsqueeze(-1)  # derivative wrt t\n",
    "\n",
    "    # PDE residual\n",
    "    return Q_t + v * Q_x - rain.unsqueeze(-1)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 4) TRAIN FUNCTION (CORRECTED 'collocation_points' REFERENCE)\n",
    "################################################################################\n",
    "def train_pinn(\n",
    "    train_data_paths,\n",
    "    test_data_path=None,\n",
    "    fraction=1.0,\n",
    "    chunk_size=500_000,\n",
    "    max_rows=None,\n",
    "    wave_speed=1.0,\n",
    "    num_epochs=500,\n",
    "    lr=1e-3,\n",
    "    pde_weight=1.0,\n",
    "    batch_size=512,\n",
    "    collocation_points=1000,  # corrected\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load training data from CSVs with chunking & optional row sampling.\n",
    "    2) Create PDE collocation points in domain ranges.\n",
    "    3) Train the PINN with data + PDE losses.\n",
    "    4) (Optionally) test on separate data set.\n",
    "\n",
    "    Args:\n",
    "      fraction (float): fraction of data rows to keep. For huge files, try <1.0\n",
    "      chunk_size (int): # of rows per chunk in read_csv\n",
    "      max_rows (int): optional total row limit per file (stops reading after this many)\n",
    "      wave_speed (float): constant v in PDE\n",
    "      num_epochs (int): training epochs\n",
    "      lr (float): learning rate\n",
    "      pde_weight (float): weighting factor for PDE residual vs data loss\n",
    "      batch_size (int): mini-batch size\n",
    "      collocation_points (int): how many PDE points to sample\n",
    "      device (str): \"cuda\" or \"cpu\"\n",
    "    \"\"\"\n",
    "    logger.info(\"=== START TRAINING PINN ===\")\n",
    "    logger.info(f\"Loading training data with fraction={fraction}, chunk_size={chunk_size}, max_rows={max_rows}\")\n",
    "    in_tensor, out_tensor = load_csv_as_dataset(train_data_paths, fraction, chunk_size, max_rows)\n",
    "    n_data = in_tensor.shape[0]\n",
    "\n",
    "    # Setup device\n",
    "    dev = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {dev}\")\n",
    "    in_tensor = in_tensor.to(dev)\n",
    "    out_tensor = out_tensor.to(dev)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(in_tensor, out_tensor)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Collocation\n",
    "    x_min, x_max = float(in_tensor[:,0].min()), float(in_tensor[:,0].max())\n",
    "    t_min, t_max = float(in_tensor[:,1].min()), float(in_tensor[:,1].max())\n",
    "    r_min, r_max = float(in_tensor[:,2].min()), float(in_tensor[:,2].max())\n",
    "    logger.info(f\"Spatial/time domain: x in [{x_min:.2f},{x_max:.2f}], t in [{t_min:.2f},{t_max:.2f}], rain in [{r_min:.2f},{r_max:.2f}]\")\n",
    "\n",
    "    # Generate random PDE collocation points within domain\n",
    "    x_col = np.random.uniform(x_min, x_max, collocation_points).astype(np.float32)\n",
    "    t_col = np.random.uniform(t_min, t_max, collocation_points).astype(np.float32)\n",
    "    r_col = np.random.uniform(r_min, r_max, collocation_points).astype(np.float32)\n",
    "    col_array = np.stack([x_col, t_col, r_col], axis=1)\n",
    "    colloc_tensor = torch.from_numpy(col_array).to(dev)\n",
    "\n",
    "    # Model\n",
    "    model = PINN(input_dim=3, hidden_dim=64, output_dim=1, num_layers=4).to(dev)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Anomaly detection\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    logger.info(f\"num_epochs={num_epochs}, lr={lr}, pde_weight={pde_weight}, batch_size={batch_size}, collocation_points={collocation_points}\")\n",
    "    logger.info(f\"wave_speed={wave_speed}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_pde_loss = 0.0\n",
    "\n",
    "        for (batch_in, batch_out) in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1) Data loss\n",
    "            Q_pred = model(batch_in)\n",
    "            data_l = loss_fn(Q_pred, batch_out)\n",
    "\n",
    "            # 2) PDE loss\n",
    "            # sample sub-batch from collocation points\n",
    "            idx = torch.randint(0, collocation_points, (batch_in.size(0),))\n",
    "            c_batch = colloc_tensor[idx, :]  # shape [batch_size, 3]\n",
    "            x_c = c_batch[:,0]\n",
    "            t_c = c_batch[:,1]\n",
    "            r_c = c_batch[:,2]\n",
    "\n",
    "            res = pde_residual(model, x_c, t_c, r_c, v=wave_speed)\n",
    "            pde_l = loss_fn(res, torch.zeros_like(res))\n",
    "\n",
    "            # total\n",
    "            total_loss = data_l + pde_weight * pde_l\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_data_loss += data_l.item() * batch_in.size(0)\n",
    "            epoch_pde_loss  += pde_l.item()  * batch_in.size(0)\n",
    "\n",
    "        epoch_data_loss /= n_data\n",
    "        epoch_pde_loss  /= n_data\n",
    "\n",
    "        # log every 50 epochs\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}] Data Loss: {epoch_data_loss:.4e}, PDE Loss: {epoch_pde_loss:.4e}\")\n",
    "\n",
    "    duration = (time.time() - start_time)/60.0\n",
    "    logger.info(f\"Training completed in {duration:.2f} minutes.\")\n",
    "\n",
    "    # (Optional) test\n",
    "    if test_data_path is not None:\n",
    "        logger.info(\"Loading test data...\")\n",
    "        test_in, test_out = load_csv_as_dataset([test_data_path], fraction=1.0, chunk_size=500_000, max_rows=None)\n",
    "        test_in = test_in.to(dev)\n",
    "        test_out = test_out.to(dev)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Q_test = model(test_in)\n",
    "            test_mse = loss_fn(Q_test, test_out).item()\n",
    "        logger.info(f\"Test MSE: {test_mse:.4e}\")\n",
    "\n",
    "    logger.info(\"=== END TRAINING PINN ===\")\n",
    "    return model\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 5) EXAMPLE USAGE\n",
    "################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Just an example set of paths (edit these to your actual CSVs)\n",
    "    train_csv_paths = [\n",
    "        \"/media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2011_merged.csv\",\n",
    "        \"/media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2012_merged.csv\",\n",
    "        \"/media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2013_merged.csv\",\n",
    "        \"/media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2014_merged.csv\",\n",
    "    ]\n",
    "    test_csv_path = \"/media/data-ssd/PINN/DATA/model data after proccessing/first expirence train on 2011 2012 2013 2014 and test on 2015/lag_model/Northern Dead Sea_2015_merged.csv\"\n",
    "\n",
    "    # Example: read only 10% of rows from each chunk, up to 500k rows per chunk,\n",
    "    # and stop after 2M total rows per file:\n",
    "    fraction_to_keep = 0.1\n",
    "    chunk_size = 500_000\n",
    "    max_rows_per_file = 2_000_000\n",
    "\n",
    "    model = train_pinn(\n",
    "        train_data_paths=train_csv_paths,\n",
    "        test_data_path=test_csv_path,\n",
    "        fraction=fraction_to_keep,\n",
    "        chunk_size=chunk_size,\n",
    "        max_rows=max_rows_per_file,\n",
    "        wave_speed=1.0,\n",
    "        num_epochs=300,   # fewer epochs for memory test\n",
    "        lr=1e-3,\n",
    "        pde_weight=1.0,\n",
    "        batch_size=512,\n",
    "        collocation_points=1000,  # note corrected usage here\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"kinematic_wave_pinn_chunked.pth\")\n",
    "    logger.info(\"Model saved to kinematic_wave_pinn_chunked.pth.\")\n",
    "\n",
    "logger.info(\"===== SINGLE-CELL SCRIPT DEFINITION COMPLETE =====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c8e21-6022-43ae-a9b8-f20361bdff29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa580aa-8e99-4a5b-8ccb-f163c4bd6d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb123a2-746f-4ae2-8fea-2a81d91d7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##sum: good run - 17 hours for 2M rows in each year. I got a model and not evaluate it on 2015. meaning it didnt save the shp file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8975acb-49df-4db1-aefd-29fdc634d5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3877e-ebb5-46e5-ba09-1979926b0955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
