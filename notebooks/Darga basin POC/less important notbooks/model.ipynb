{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d6b300-c229-4cc7-8e9b-b86028a313fc",
   "metadata": {},
   "source": [
    "Professional Prompt for PINN-Based Discharge Prediction Using PyTorch (RTX 6000 GPU)\n",
    "Objective:\n",
    "We aim to develop a Physics-Informed Neural Network (PINN) using PyTorch to predict discharge (rg_qms) at hydrometric stations based on rainfall, spatial terrain data, and temporal evolution. The model will be trained on historical data from multiple years, tested on available years, and run efficiently on an NVIDIA RTX 6000 GPU using batch processing and real-time monitoring.\n",
    "\n",
    "Dataset Description & File Paths:\n",
    "The dataset is structured as merged hydrological data containing spatiotemporal features. The data is stored in the following directory:\n",
    "ðŸ“‚ Path:\n",
    "/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\n",
    "\n",
    "ðŸ“„ File Names:\n",
    "\n",
    "Merged_North_Dead_Sea_2012.csv\n",
    "Merged_North_Dead_Sea_2013.csv\n",
    "Merged_North_Dead_Sea_2014.csv\n",
    "Merged_North_Dead_Sea_2015.csv\n",
    "Merged_North_Dead_Sea_2016.csv\n",
    "Merged_North_Dead_Sea_2018.csv\n",
    "Merged_North_Dead_Sea_2019.csv\n",
    "Merged_North_Dead_Sea_2020.csv\n",
    "ðŸ”¹ Columns Available:\n",
    "\n",
    "Time Variables:\n",
    "\n",
    "time (Datetime) â€“ Timestamp of the observation.\n",
    "Spatial Features:\n",
    "\n",
    "longitude, latitude (Float) â€“ Spatial coordinates of each observation.\n",
    "elevation (Float) â€“ Elevation of each grid point.\n",
    "slope_calculated (Float) â€“ Terrain slope, useful for estimating flow direction.\n",
    "Hydrometeorological Inputs:\n",
    "\n",
    "rainrate (Float) â€“ Rainfall intensity at each location.\n",
    "rg_qms (Float) â€“ Discharge at specific hydrometric stations (our target variable).\n",
    "Data Gaps:\n",
    "\n",
    "rg_qms is only available for three specific station locations per time step.\n",
    "Other locations contain NaN values.\n",
    "Task for the PINN Model:\n",
    "The PINN will be trained to:\n",
    "\n",
    "Learn the relationship between past rainfall and discharge using historical years as training data.\n",
    "Enforce physics constraints using governing equations like the kinematic wave equation.\n",
    "Predict rg_qms at future timestamps based on rainfall inputs and spatial information.\n",
    "Generalize across different years, testing model robustness.\n",
    "Model Architecture & Training Strategy (PyTorch PINN)\n",
    "1ï¸âƒ£ Input Features:\n",
    "Time-dependent inputs: rainrate, time, historical discharge values (if available).\n",
    "Spatial inputs: longitude, latitude, elevation, slope_calculated.\n",
    "2ï¸âƒ£ PINN Constraints (Loss Functions)\n",
    "Data Loss (L_data): Mean Squared Error (MSE) on discharge predictions.\n",
    "Physics Residual Loss (L_physics):\n",
    "Continuity Equation: Ensures smooth discharge variations over space and time.\n",
    "Momentum Conservation: Captures flow behavior based on terrain slope.\n",
    "Final Loss Function:\n",
    "ð¿\n",
    "=\n",
    "ð¿\n",
    "data\n",
    "+\n",
    "ðœ†\n",
    "ð¿\n",
    "physics\n",
    "L=L \n",
    "data\n",
    "â€‹\n",
    " +Î»L \n",
    "physics\n",
    "â€‹\n",
    " \n",
    "where Î» is a weighting parameter to balance data and physics loss.\n",
    "3ï¸âƒ£ Model Architecture (PyTorch)\n",
    "Fully Connected Neural Network (FCNN)\n",
    "Layers: 4-6 layers with Sine Activation (SinAct) for handling physics constraints.\n",
    "Neurons: 128-256 per layer.\n",
    "Batch Normalization: Improves training stability.\n",
    "Dropout Layers: Reduces overfitting.\n",
    "4ï¸âƒ£ Training & GPU Optimization\n",
    "Hardware: NVIDIA RTX 6000 GPU with CUDA acceleration.\n",
    "Batch Processing: Uses mini-batches of 256 or 512 samples for efficient memory usage.\n",
    "Optimization Strategy:\n",
    "Adam optimizer for fast convergence.\n",
    "L-BFGS optimizer for final refinement.\n",
    "Gradient Clipping: Prevents instability in training.\n",
    "Training & Validation Plan\n",
    "Train on: 2012, 2013, 2014, 2015.\n",
    "Test on: 2016, 2018, 2019, 2020.\n",
    "Evaluation Metrics:\n",
    "Root Mean Squared Error (RMSE).\n",
    "Nash-Sutcliffe Efficiency (NSE).\n",
    "RÂ² Score to measure prediction accuracy.\n",
    "Tracking Model Progress\n",
    "âœ… Live Loss Monitoring: Logs loss function values (data loss + physics loss) during training.\n",
    "âœ… Batch-wise Validation: Computes validation loss every N batches for real-time progress tracking.\n",
    "âœ… TensorBoard Integration: If needed, to visualize training curves.\n",
    "âœ… Real-Time Model Checkpoints: Saves the best model based on validation loss.\n",
    "\n",
    "Deliverables\n",
    "A fully trained PINN model capable of predicting discharge (rg_qms).\n",
    "Validation plots comparing model predictions vs. observed discharge.\n",
    "Physics-based insights into water flow behavior.\n",
    "Generalization analysis across different years.\n",
    "Final Request\n",
    "Implement this using PyTorch + CUDA, utilizing batch-based training on the RTX 6000 GPU. Ensure real-time progress tracking and visualization of predictions vs. observed data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "415e64d9-143a-44c5-ba4f-556a2ed258eb",
   "metadata": {},
   "source": [
    "1. ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ×•×”×›× ×” (Data Loading & Preprocessing)\n",
    "×ž×” ×”×§×•×“ ×¢×•×©×”:\n",
    "\n",
    "×˜×¢×™× ×ª ×§×‘×¦×™ CSV:\n",
    "×”×§×•×“ ×¢×•×‘×¨ ×¢×œ ×§×‘×¦×™× ×©×ž×›×™×œ×™× × ×ª×•× ×™× (×œ×ž×©×œ, × ×ª×•× ×™ ×ž×–×’ ××•×•×™×¨ ×•×ž×“×™×“×•×ª) ×¢×‘×•×¨ ×©× ×™× ×©×•× ×•×ª (×œ×ž×©×œ 2012, 2013 ×•×›×•').\n",
    "×”×•× ×ž×©×ª×ž×© ×‘× ×ª×™×‘ (×ž×™×§×•× ×‘×ª×™×§×™×™×”) ×›×“×™ ×œ×ž×¦×•× ××ª ×”×§×‘×¦×™× ×•×ž×™×™×‘× ××•×ª× ×œ×ª×•×š ×˜×‘×œ××•×ª (DataFrames) ×‘×¢×–×¨×ª ×¡×¤×¨×™×™×ª Pandas.\n",
    "\n",
    "×”×¤×—×ª×ª ×©×™×ž×•×© ×‘×–×™×›×¨×•×Ÿ:\n",
    "×›××©×¨ ×ž×™×™×‘××™× ××ª ×”×§×‘×¦×™×, ×”×§×•×“ ×ž×’×“×™×¨ ×¡×•×’×™ × ×ª×•× ×™× (×›×ž×• np.float32 ×‘×ž×§×•× float64) ×›×“×™ ×œ×—×¡×•×š ×‘×–×™×›×¨×•×Ÿ ×”×ž×—×©×‘.\n",
    "\n",
    "×¢×™×‘×•×“ ×ª×›×•× ×•×ª ×”×–×ž×Ÿ:\n",
    "×”× ×ª×•× ×™× ×ž×›×™×œ×™× ×’× ×¢×ž×•×“×ª ×–×ž×Ÿ. ×”×§×•×“ ×ž×—×©×‘ â€œ×©×¢×”â€ ×‘×¦×•×¨×” ×¢×©×¨×•× ×™×ª (×›×ž×• 13.5 ×¢×‘×•×¨ 13:30) ×•×ž×ž×™×¨ ××ª ×”×©×¢×” ×”×–×• ×œ×©×ª×™ ×ª×›×•× ×•×ª â€“ ××—×ª ×©×ž×©×ª×ž×©×ª ×‘×¤×•× ×§×¦×™×™×ª ×¡×™× ×•×¡ ×•××—×ª ×‘×§×•×¡×™× ×•×¡.\n",
    "×–×” × ×¢×©×” ×ž×›×™×•×•×Ÿ ×©×”×–×ž×Ÿ ×”×•× ×ž×—×–×•×¨×™ (×©×¢×•×ª ×”×™×•× ×—×•×–×¨×•×ª) ×•×”×™×™×¦×•×’ ×”×¡×™× ×•×¡×™/×§×•×¡×™× ×•×¡×™ ×¢×•×–×¨ ×œ×ž×•×“×œ ×œ×”×‘×™×Ÿ ××ª ×”×ž×—×–×•×¨×™×•×ª ×”×–×•.\n",
    "\n",
    "× ×¨×ž×•×œ (Scaling):\n",
    "×”× ×ª×•× ×™× ×¢×•×‘×¨×™× ×ª×”×œ×™×š ×©×œ × ×¨×ž×•×œ â€“ ×›×œ×•×ž×¨, ×›×œ ×”×¢×¨×›×™× ×ž×•×ž×¨×¦×™× ×›×š ×©×™×”×™×• ×‘×˜×•×•×— ×“×•×ž×”. ×–×” ×¢×•×–×¨ ×œ×ž×•×“×œ ×œ×œ×ž×•×“ ×‘×¦×•×¨×” ×™×¦×™×‘×” ×™×•×ª×¨.\n",
    "\n",
    "×¢×™×‘×•×“ ×”×¢×¨×š ×”×ž×˜×¨×” (Target):\n",
    "×”×¢×¨×š ××•×ª×• ×× ×• ×¨×•×¦×™× ×œ× ×‘× (×œ×ž×©×œ, × ×¤×™×œ×ª ×ž×™×) ×¢×•×‘×¨ ×’× ×”×•× × ×¨×ž×•×œ, ××š ×¨×§ ×¢×‘×•×¨ ×”×©×•×¨×•×ª ×‘×”×Ÿ ×™×© ×¢×¨×š ×ª×§×£.\n",
    "\n",
    "2. ×™×¦×™×¨×ª ×ž×¢×¨×š × ×ª×•× ×™× (Dataset) ×•×˜×•×¢×Ÿ × ×ª×•× ×™× (DataLoader)\n",
    "×ž×” ×”×§×•×“ ×¢×•×©×”:\n",
    "\n",
    "×™×¦×™×¨×ª ×ž×—×œ×§×ª Dataset:\n",
    "×›××Ÿ × ×‘× ×™×ª ×ž×—×œ×§×” ×©×ž×™×™×¦×’×ª ××ª ×”× ×ª×•× ×™× ×©×œ× ×• ×›×š ×©× ×™×ª×Ÿ ×™×”×™×” ×œ×§×‘×œ ×“×•×’×ž×” ××—×ª ×‘×›×œ ×¤×¢× (×œ×“×•×’×ž×”, ×©×•×¨×” ××—×ª ×ž×”× ×ª×•× ×™×).\n",
    "×”×ž×—×œ×§×” â€œHydrologyDatasetâ€ ×ž××—×¡× ×ª ××ª ×”×ª×›×•× ×•×ª (×›×ž×• ×–×ž×Ÿ, ×§×•××•×¨×“×™× ×˜×•×ª ×•×›×•') ×•××ª ×”×¢×¨×›×™× ×©×× ×• ×¨×•×¦×™× ×œ× ×‘×.\n",
    "\n",
    "DataLoader:\n",
    "×‘×¢×–×¨×ª DataLoader, ×× ×—× ×• ×™×•×¦×¨×™× â€œ×§×‘×•×¦×•×ªâ€ (batches) ×©×œ × ×ª×•× ×™×. ×”×§×•×“ ×™×•×¦×¨ ×˜×•×¢×Ÿ × ×ª×•× ×™× ×©×ž×—×œ×§ ××ª ×”× ×ª×•× ×™× ×œ×§×‘×•×¦×•×ª ×§×˜× ×•×ª ×™×•×ª×¨ (×œ×ž×©×œ 64 ×“×•×’×ž××•×ª ×‘×›×œ ×¤×¢×).\n",
    "×›××©×¨ ×¢×•×‘×“×™× ×¢× GPU, ×”×§×•×“ ×’× ×ž×’×“×™×¨ ××ª ×”××¤×©×¨×•×ª ×©×œ â€œpin_memoryâ€ ×œ×©×™×¤×•×¨ ×”×¢×‘×¨×ª ×”× ×ª×•× ×™× ×ž×”×–×™×›×¨×•×Ÿ ×”×¨××©×™ ×œ×–×™×›×¨×•×Ÿ ×©×œ ×”-GPU.\n",
    "\n",
    "3. ×”×’×“×¨×ª ×”×ž×•×“×œ (Model Definition)\n",
    "×ž×” ×”×§×•×“ ×¢×•×©×”:\n",
    "\n",
    "×ž×•×“×œ ×¨×©×ª ×¢×¦×‘×™×ª (Neural Network):\n",
    "×”×ž×•×“×œ ×”×•× ×¨×©×ª ×¢×¦×‘×™×ª ×ž×œ××” (Fully Connected Neural Network) ×©× ×‘× ×ª×” ×œ×©× × ×™×‘×•×™ ×”×¢×¨×š ×”×ž×˜×¨×”.\n",
    "×”×¨×©×ª ×ž×•×¨×›×‘×ª ×ž×ž×¡×¤×¨ ×©×›×‘×•×ª:\n",
    "\n",
    "×©×›×‘×ª ×§×œ×˜ (Input Layer):\n",
    "×ž×§×‘×œ×ª ××ª ×›×œ ×”×ª×›×•× ×•×ª ×©×”×’×“×¨× ×• (×œ×ž×©×œ ×–×ž×Ÿ, ×§×•××•×¨×“×™× ×˜×•×ª, ×’×•×‘×”, ×•×›×•').\n",
    "×©×›×‘×•×ª × ×¡×ª×¨×•×ª (Hidden Layers):\n",
    "×™×© ×›×ž×” ×©×›×‘×•×ª ×‘×”×Ÿ ×›×œ ××—×ª ×ž×‘×¦×¢×ª ×—×™×©×•×‘×™× ×¢×œ ×”× ×ª×•× ×™× ×•×ž×¢×‘×™×¨×” ××ª ×”×ª×•×¦××” ×œ×©×›×‘×” ×”×‘××”.\n",
    "×‘×›×œ ×©×›×‘×” ×ž×•×¤×¢×œ×ª ×¤×•× ×§×¦×™×™×ª ×”×¤×¢×œ×” ×ž×¡×•×’ ×¡×™× ×•×¡ (Sine), ×©×”×™× ×œ× ×¡×˜× ×“×¨×˜×™×ª ×›×ž×• ReLU ××• Sigmoid, ××š ×›××Ÿ ×”×™× × ×‘×—×¨×” ×ž×›×™×•×•×Ÿ ×©×”×™× ×™×›×•×œ×” ×œ×¢×–×•×¨ ×œ×œ×›×•×“ ×“×¤×•×¡×™× ×ž×—×–×•×¨×™×™× (×›×ž×• ×ž×—×–×•×¨×™ ×–×ž×Ÿ).\n",
    "×©×›×‘×ª ×¤×œ×˜ (Output Layer):\n",
    "×ž× ×™×‘×” ××ª ×”× ×™×‘×•×™ â€“ ×‘×ž×§×¨×” ×–×” ×¢×¨×š × ×™×‘×•×™ ××—×“ (×œ×“×•×’×ž×”, × ×¤×™×œ×ª ×ž×™×).\n",
    "×”×¢×‘×¨×ª ×”×ž×•×“×œ ×œ-GPU:\n",
    "×× ×™×© GPU ×–×ž×™×Ÿ, ×”×§×•×“ ×ž×¢×‘×™×¨ ××ª ×”×ž×•×“×œ ×œ×©×™×ž×•×© ×‘-GPU ×›×“×™ ×œ×”××™×¥ ××ª ×”×—×™×©×•×‘×™×.\n",
    "\n",
    "4. ×¤×•× ×§×¦×™×•×ª ××•×‘×“×Ÿ (Loss Functions) ×•××™×œ×•×¦×™× ×¤×™×–×™×§×œ×™×™×\n",
    "×ž×” ×”×§×•×“ ×¢×•×©×”:\n",
    "\n",
    "××•×‘×“×Ÿ × ×ª×•× ×™× (Data Loss):\n",
    "×¤×•× ×§×¦×™×™×ª ××•×‘×“×Ÿ ×ž×¡×•×’ MSE (Mean Squared Error) ×ž×—×©×‘×ª ××ª ×”×”×‘×“×œ ×‘×™×Ÿ ×”×¢×¨×š ×”× ×™×‘× ×œ×¢×¨×š ×”××ž×™×ª×™, ××š ×”×™× ×ž×—×©×‘×ª ×–××ª ×¨×§ ×¢×‘×•×¨ ×”×©×•×¨×•×ª ×‘×”×Ÿ ×™×© × ×ª×•× ×™× ×ª×§×¤×™× (×œ×ž×©×œ, ×¨×§ ×¢×‘×•×¨ ×”×ž×§×•×ž×•×ª ×‘×”× ×™×© ×ž×“×™×“×•×ª).\n",
    "\n",
    "××•×‘×“×Ÿ ×¤×™×–×™×§×œ×™ (Physics Loss):\n",
    "×‘× ×•×¡×£ ×œ××•×‘×“×Ÿ ×”× ×ª×•× ×™×, ×™×©× ×• ×ž×¨×›×™×‘ × ×•×¡×£ ×©×œ ××•×‘×“×Ÿ ×©×ž× ×¡×” ×œ×•×•×“× ×©×”× ×™×‘×•×™ ×ž×©×ª× ×” ×‘×¦×•×¨×” ×—×œ×§×” ×‘×™×—×¡ ×œ×ž×©×ª× ×” ×ž×¨×—×‘×™ (×œ×ž×©×œ, ×œ×¤×™ ×§×• ××•×¨×š).\n",
    "×–×” × ×¢×©×” ×¢×œ ×™×“×™ ×—×™×©×•×‘ × ×’×–×¨×ª (gradient) â€“ ×›×œ×•×ž×¨, ×œ×‘×“×•×§ ×›×ž×” ×ž×”×¨ ×ž×©×ª× ×” ×”×¢×¨×š ×”× ×™×‘× ×›××©×¨ ×ž×©× ×™× ××ª ×§×• ×”××•×¨×š, ×•×œ×•×•×“× ×©×–×” ×œ× ×ž×©×ª× ×” ×‘×¤×ª××•×ž×™×•×ª.\n",
    "\n",
    "×©×™×œ×•×‘ ×©× ×™ ×¡×•×’×™ ×”××•×‘×“×Ÿ:\n",
    "×”×¡×š ×”×›×œ ×©×œ ×”××•×‘×“×Ÿ ×”×•× ×¡×›×•× ×©×œ ××•×‘×“×Ÿ ×”× ×ª×•× ×™× ×•×”××•×‘×“×Ÿ ×”×¤×™×–×™×§×œ×™, ×›××©×¨ × ×™×ª×Ÿ ×œ×ª×ª ×ž×©×§×œ (Î») ×œ××•×‘×“×Ÿ ×”×¤×™×–×™×§×œ×™ ×›×“×™ ×œ×©× ×•×ª ××ª ×”×”×©×¤×¢×” ×©×œ×•.\n",
    "\n",
    "5. ×”×’×“×¨×ª ×”××•×¤×˜×™×ž×™×™×–×¨, ×©×ž×™×¨×ª ×”×ª×§×“×ž×•×ª (Checkpointing) ×•×¨×™×©×•× (Logging)\n",
    "×ž×” ×”×§×•×“ ×¢×•×©×”:\n",
    "\n",
    "××•×¤×˜×™×ž×™×™×–×¨:\n",
    "×”××•×¤×˜×™×ž×™×™×–×¨ (×‘×ž×§×¨×” ×–×” Adam) ×”×•× ××œ×’×•×¨×™×ª× ×©×ž×¢×“×›×Ÿ ××ª ×”×ž×©×§×œ×™× ×©×œ ×”×¨×©×ª ×”×¢×¦×‘×™×ª ×¢×œ ×ž× ×ª ×œ×”×¤×—×™×ª ××ª ×”××•×‘×“×Ÿ.\n",
    "×”×•× ×¢×•×‘×¨ ×¢×œ ×”× ×ª×•× ×™×, ×ž×—×©×‘ ××ª ×”××•×‘×“×Ÿ, ×•×ž×©× ×” ××ª ×”×ž×©×§×œ×™× ×©×œ ×”×ž×•×“×œ ×‘×”×ª××.\n",
    "\n",
    "×©×ž×™×¨×ª ×”×ª×§×“×ž×•×ª (Checkpointing):\n",
    "×”×§×•×“ ×‘×•×“×§ ×× ×™×© â€œ× ×§×•×“×ª ×©×ž×™×¨×”â€ (checkpoint) ×§×•×“×ž×ª. ×× ×›×Ÿ, ×”×•× ×˜×•×¢×Ÿ ××ª ×”×ž×•×“×œ ×•××ª ×ž×¦×‘ ×”××•×¤×˜×™×ž×™×™×–×¨ ×•×ž×ž×©×™×š ××ª ×”××™×ž×•×Ÿ ×ž×”× ×§×•×“×” ×”××—×¨×•× ×”.\n",
    "×‘×¡×™×•× ×›×œ ×¢×™×“×Ÿ (epoch), ×”×§×•×“ ×©×•×ž×¨ ××ª ×ž×¦×‘ ×”×ž×•×“×œ, ×›×š ×©×× ×ž×©×”×• ×™×©×ª×‘×©, × ×™×ª×Ÿ ×œ×”×ž×©×™×š ×ž×”× ×§×•×“×” ×”××—×¨×•× ×”.\n",
    "\n",
    "×¨×™×©×•× (Logging):\n",
    "×”×§×•×“ ×ž×©×ª×ž×© ×‘×›×œ×™ ×¨×™×©×•× (TensorBoard) ×•×›×Ÿ ×ž×“×¤×™×¡ ×”×•×“×¢×•×ª ×¢× ×ª××¨×™×š ×•×©×¢×” ×‘×›×œ ×©×œ×‘ ×—×©×•×‘ (×›×ž×• ×˜×¢×™× ×ª × ×ª×•× ×™×, ×¡×™×•× ×¢×™×“×Ÿ, ×•×›×•') ×›×“×™ ×œ×¢×§×•×‘ ××—×¨×™ ×ž×” ×©×§×•×¨×” ×‘×–×ž×Ÿ ×”××™×ž×•×Ÿ.\n",
    "\n",
    "6. ×œ×•×œ××ª ×”××™×ž×•×Ÿ (Training Loop)\n",
    "×ž×” ×”×§×•×“ ×¢×•×©×”:\n",
    "\n",
    "×ª×—×™×œ×ª ×”××™×ž×•×Ÿ:\n",
    "×”×œ×•×œ××” ×¢×•×‘×¨×ª ×¢×œ ×ž×¡×¤×¨ ×¢×™×“× ×™× (epochs) â€“ ×›×œ×•×ž×¨, ×”×™× ×¢×•×‘×¨×ª ×¢×œ ×›×œ ×”× ×ª×•× ×™× ×ž×¡×¤×¨ ×¤×¢×ž×™×.\n",
    "\n",
    "×¢×‘×•×¨ ×›×œ ×¢×™×“×Ÿ:\n",
    "×”×§×•×“ ×ž×•×“×“ ××ª ×”×–×ž×Ÿ ×©×”×¢×™×“×Ÿ ×œ×§×—, ×ž×—×©×‘ ××ª ×”××•×‘×“×Ÿ ×”×›×•×œ×œ, ×•×ž×“×•×•×— ×’× ×¢×œ ×–×ž×Ÿ ×ž×©×•×¢×¨ ×œ×¡×™×•× ×”××™×ž×•×Ÿ.\n",
    "\n",
    "×¨×™×©×•× ×‘×›×œ ×¢×™×“×Ÿ:\n",
    "×‘×¡×™×•× ×›×œ ×¢×™×“×Ÿ, ×”×§×•×“ ×ž×—×©×‘ ××ª ×ž×ž×•×¦×¢ ×”××•×‘×“×Ÿ ×•×ž×“×¤×™×¡ ×”×•×“×¢×•×ª ×¢× ×”×–×ž×Ÿ ×©×œ ×”×¢×™×“×Ÿ, ×”××•×‘×“×Ÿ, ×•×”×¢×¨×›×ª ×”×–×ž×Ÿ ×©× ×•×ª×¨ ×œ××™×ž×•×Ÿ.\n",
    "\n",
    "×‘×“×™×§×ª ×‘×™×¦×•×¢×™× (Validation):\n",
    "×‘×›×œ ×¢×™×“×Ÿ, ×œ××—×¨ ×”××™×ž×•×Ÿ, ×”×§×•×“ ×ž×¨×™×¥ ××ª ×”×ž×•×“×œ ×¢×œ × ×ª×•× ×™ ×‘×“×™×§×” (test data) ×•×ž×—×©×‘ ××ª ×”××•×‘×“×Ÿ. ×–×” ×¢×•×–×¨ ×œ× ×• ×œ×“×¢×ª ×× ×”×ž×•×“×œ ×ž×©×ª×¤×¨ ×’× ×¢×œ × ×ª×•× ×™× ×©×œ× × ×¨××• ×§×•×“×.\n",
    "\n",
    "×©×ž×™×¨×ª × ×§×•×“×•×ª ×©×ž×™×¨×” (Checkpoint):\n",
    "×× ×”××•×‘×“×Ÿ ×‘×‘×“×™×§×” ×˜×•×‘ ×™×•×ª×¨ ×ž×”×ž×¦×‘ ×”×˜×•×‘ ×‘×™×•×ª×¨ ×”×§×•×“×, ×”×§×•×“ ×©×•×ž×¨ ××ª ×ž×¦×‘ ×”×ž×•×“×œ ×œ×¢×™×“×Ÿ ×–×”.\n",
    "\n",
    "×ž×¢×§×‘ ××—×¨×™ ×–×™×›×¨×•×Ÿ GPU:\n",
    "×× ×”×ž×•×“×œ ×¨×¥ ×¢×œ GPU, ×”×§×•×“ ×’× ×ž×“×¤×™×¡ ×›×ž×” ×–×™×›×¨×•×Ÿ ×ž×•×§×¦×” ×•× ×ž×¦× ×‘×©×™×ž×•×©.\n",
    "\n",
    "7. ×©×ž×™×¨×ª ×”×ž×•×“×œ ×”×¡×•×¤×™ ×•×¡×™×•×\n",
    "×ž×” ×”×§×•×“ ×¢×•×©×”:\n",
    "\n",
    "×‘×¡×™×•× ×›×œ ×”××™×ž×•× ×™×, ×”×§×•×“ ×©×•×ž×¨ ××ª ×”×ž×•×“×œ ×”×¡×•×¤×™ (×ž×©×§×œ×™ ×”×¨×©×ª) ×œ×§×•×‘×¥.\n",
    "×”×§×•×“ ×¡×•×’×¨ ×’× ××ª ×›×œ×™ ×”×¨×™×©×•× (TensorBoard).\n",
    "×¡×™×›×•× ×›×œ×œ×™\n",
    "×˜×¢×™× ×ª ×”× ×ª×•× ×™×:\n",
    "×”×§×•×“ ×˜×•×¢×Ÿ ×•×ž×¢×‘×“ ××ª ×”× ×ª×•× ×™×, ×ž×ž×™×¨ ××•×ª× ×œ×¤×•×¨×ž×˜ × ×•×— ×œ×¢×‘×•×“×” ×•×ž× ×¨×ž×œ ××•×ª×.\n",
    "\n",
    "×”×›× ×ª ×”× ×ª×•× ×™× ×œ××™×ž×•×Ÿ:\n",
    "× ×‘× ×™×ª ×ž×—×œ×§×ª Dataset ×•-DataLoader ×›×“×™ ×œ×˜×¢×•×Ÿ ××ª ×”× ×ª×•× ×™× ×‘×§×‘×•×¦×•×ª (batches) ×§×˜× ×•×ª, ×ž×” ×©×ž××¤×©×¨ ×—×™×©×•×‘ ×™×¢×™×œ ×™×•×ª×¨.\n",
    "\n",
    "×”×’×“×¨×ª ×”×ž×•×“×œ:\n",
    "×ž×•×’×“×¨×ª ×¨×©×ª ×¢×¦×‘×™×ª ×©×ž×•×¨×›×‘×ª ×ž×©×›×‘×•×ª ×§×œ×˜, ×©×›×‘×•×ª × ×¡×ª×¨×•×ª ×¢× ×¤×•× ×§×¦×™×•×ª ×”×¤×¢×œ×” ×¡×™× ×•×¡×™×•×ª ×•×©×›×‘×ª ×¤×œ×˜ ×©×ž× ×‘××ª ××ª ×”×¢×¨×š ×”×¨×¦×•×™.\n",
    "\n",
    "×—×™×©×•×‘ ×”××•×‘×“×Ÿ:\n",
    "×ž×—×•×©×‘ ××•×‘×“×Ÿ × ×ª×•× ×™× (×ž×¡×ª×ž×š ×¢×œ ×”×‘×“×œ×™× ×‘×™×Ÿ × ×™×‘×•×™ ×œ×¢×¨×š ××ž×™×ª×™) ×™×—×“ ×¢× ××•×‘×“×Ÿ ×©×ž×•×•×“× ×©×”×ª×•×¦××” ×ž×©×ª× ×” ×‘×¦×•×¨×” ×—×œ×§×”.\n",
    "\n",
    "××•×¤×˜×™×ž×™×™×–×¦×™×” ×•×©×ž×™×¨×ª ×”×ª×§×“×ž×•×ª:\n",
    "×”××•×¤×˜×™×ž×™×™×–×¨ ×ž×©× ×” ××ª ×ž×©×§×œ×™ ×”×ž×•×“×œ, ×•×”×§×•×“ ×©×•×ž×¨ × ×§×•×“×•×ª ×©×ž×™×¨×” ×›×“×™ ×©×ª×•×›×œ ×œ×”×ž×©×™×š ××ª ×”××™×ž×•×Ÿ ×× ×ª×¤×¡×™×§ ××• ×× ×™×© ×ª×§×œ×”.\n",
    "\n",
    "×œ×•×œ××ª ×”××™×ž×•×Ÿ:\n",
    "×”×§×•×“ ×¢×•×‘×¨ ×¢×œ ×”× ×ª×•× ×™× ×‘×ž×¡×¤×¨ ×¢×™×“× ×™×, ×ž×—×©×‘ ×•×ž×“×•×•×— ×¢×œ ×”×‘×™×¦×•×¢×™× ×‘×›×œ ×¢×™×“×Ÿ, ×•×©×•×ž×¨ ××ª ×”×ž×¦×‘ ×‘×ž×™×“×” ×•×”×‘×™×¦×•×¢×™× ×ž×©×ª×¤×¨×™×.\n",
    "\n",
    "×”×¡×™×•×:\n",
    "×œ××—×¨ ×¡×™×•× ×”××™×ž×•×Ÿ, ×”×ž×•×“×œ ×”×¡×•×¤×™ × ×©×ž×¨ ×•×”×›×œ × ×¡×’×¨ ×‘×¦×•×¨×” ×ž×¡×•×“×¨×ª.\n",
    "\n",
    "×”×ž×˜×¨×” ×©×œ ×”×§×•×“ ×”×™× ×œ××ž×Ÿ ×ž×•×“×œ ×©×™×›×•×œ ×œ× ×‘× ×¢×¨×š (×›×ž×• × ×¤×™×œ×ª ×ž×™×) ×ª×•×š ×©×ž×™×¨×” ×¢×œ ×™×—×¡×™× ×¤×™×–×™×§×œ×™×™× (×›×ž×• ×—×œ×§×•×ª ×©×™× ×•×™ ×œ×¤×™ ×§×• ××•×¨×š) ×•××•×¤×˜×™×ž×™×–×¦×™×” ×ª×•×š ×©×™×ž×•×© ×‘-GPU ×›××©×¨ ×”×•× ×–×ž×™×Ÿ. ×”×§×•×“ ×’× ×“×•××’ ×œ×©×ž×•×¨ ××ª ×”×”×ª×§×“×ž×•×ª ×›×š ×©×ª×•×›×œ ×œ×”×¤×¡×™×§ ×•×œ×”×ž×©×™×š ×‘××™×ž×•×Ÿ ×ž×‘×œ×™ ×œ××‘×“ ××ª ×ž×” ×©×œ×ž×“ ×”×ž×•×“×œ ×¢×“ ×›×”.\n",
    "\n",
    "×× ×™ ×ž×§×•×•×” ×©×”×”×¡×‘×¨ ×‘×¨×•×¨! ×× ×™×© ×œ×š ×©××œ×•×ª × ×•×¡×¤×•×ª, ×× ×™ ×›××Ÿ ×œ×¢×–×•×¨."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8c4a8b2-3fde-49b4-a57e-8a2bf5a21df0",
   "metadata": {},
   "source": [
    "o3-mini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e93ace-9039-4fff-8f56-1c5692c335cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 09:34:14.889527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 09:34:17] Starting data loading...\n",
      "[2025-02-18 09:34:17] Loading training data...\n",
      "[2025-02-18 09:34:17] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2012.csv\n",
      "[2025-02-18 09:35:39] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2013.csv\n",
      "[2025-02-18 09:36:58] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2014.csv\n",
      "[2025-02-18 09:38:19] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2015.csv\n",
      "[2025-02-18 09:39:37] Loading testing data...\n",
      "[2025-02-18 09:39:37] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2016.csv\n",
      "[2025-02-18 09:40:54] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2018.csv\n",
      "[2025-02-18 09:42:15] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2019.csv\n",
      "[2025-02-18 09:43:32] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2020.csv\n",
      "[2025-02-18 09:44:53] Data loading complete.\n",
      "[2025-02-18 09:44:53] Starting time feature encoding...\n",
      "[2025-02-18 09:44:53] Encoding training data time features...\n",
      "[2025-02-18 09:45:03] Encoding testing data time features...\n",
      "[2025-02-18 09:45:12] Starting feature scaling...\n",
      "[2025-02-18 09:46:02] Feature scaling complete.\n",
      "[2025-02-18 09:46:02] Scaling target values for valid measurements...\n",
      "[2025-02-18 09:46:02] Data preprocessing complete.\n",
      "[2025-02-18 09:46:02] Building custom dataset and dataloaders...\n",
      "[2025-02-18 09:46:10] Dataset and dataloaders ready.\n",
      "[2025-02-18 09:46:10] Building model...\n",
      "[2025-02-18 09:46:10] Model built and moved to device.\n",
      "[2025-02-18 09:46:10] Setting up loss functions...\n",
      "[2025-02-18 09:46:10] Loss functions set up.\n",
      "[2025-02-18 09:46:10] Setting up optimizer and logging...\n",
      "[2025-02-18 09:46:11] Training setup complete.\n",
      "[2025-02-18 09:46:11] Starting training loop...\n",
      "[2025-02-18 09:46:11] Epoch 1/50 started.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 229\u001b[0m\n\u001b[1;32m    227\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    228\u001b[0m loss, data_loss_val, phys_loss_val \u001b[38;5;241m=\u001b[39m combined_loss(model, x_batch, y_batch)\n\u001b[0;32m--> 229\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n\u001b[1;32m    231\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =============================\n",
    "# Logging Helper Function\n",
    "# =============================\n",
    "def log_message(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "# =============================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# =============================\n",
    "log_message(\"Starting data loading...\")\n",
    "\n",
    "TRAIN_YEARS = ['2012', '2013', '2014', '2015']\n",
    "TEST_YEARS  = ['2016', '2018', '2019', '2020']\n",
    "\n",
    "DATA_DIR = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\"\n",
    "\n",
    "# Specify data types to reduce memory usage.\n",
    "# Adjust the types based on your actual data.\n",
    "dtypes = {\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'rainrate': np.float32,\n",
    "    'elevation': np.float32,\n",
    "    'slope_calculated': np.float32\n",
    "    # You can add other columns if needed.\n",
    "}\n",
    "\n",
    "def load_data(years):\n",
    "    df_list = []\n",
    "    for yr in years:\n",
    "        pattern = os.path.join(DATA_DIR, f\"*_{yr}.csv\")\n",
    "        for file in glob.glob(pattern):\n",
    "            log_message(f\"Loading file: {file}\")\n",
    "            # Use the dtypes dictionary and parse dates\n",
    "            df = pd.read_csv(file, parse_dates=['time'], dtype=dtypes)\n",
    "            df_list.append(df)\n",
    "    data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "log_message(\"Loading training data...\")\n",
    "train_df = load_data(TRAIN_YEARS)\n",
    "log_message(\"Loading testing data...\")\n",
    "test_df  = load_data(TEST_YEARS)\n",
    "log_message(\"Data loading complete.\")\n",
    "\n",
    "# Uncomment below for debugging with a smaller subset.\n",
    "# log_message(\"Using a small subset of data for debugging.\")\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "log_message(\"Starting time feature encoding...\")\n",
    "\n",
    "time_col     = 'time'\n",
    "spatial_cols = ['longitude', 'latitude', 'elevation', 'slope_calculated']\n",
    "hydro_cols   = ['rainrate']\n",
    "target_col   = 'rg_qms'\n",
    "\n",
    "def encode_time(df):\n",
    "    df['hour'] = df['time'].dt.hour + df['time'].dt.minute / 60.0 + df['time'].dt.second / 3600.0\n",
    "    df['time_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    return df\n",
    "\n",
    "log_message(\"Encoding training data time features...\")\n",
    "train_df = encode_time(train_df)\n",
    "log_message(\"Encoding testing data time features...\")\n",
    "test_df  = encode_time(test_df)\n",
    "\n",
    "feature_cols = ['time_sin', 'time_cos'] + spatial_cols + hydro_cols\n",
    "\n",
    "log_message(\"Starting feature scaling...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df[feature_cols]  = scaler.transform(test_df[feature_cols])\n",
    "log_message(\"Feature scaling complete.\")\n",
    "\n",
    "log_message(\"Scaling target values for valid measurements...\")\n",
    "target_scaler = StandardScaler()\n",
    "valid_train_mask = train_df[target_col].notna()\n",
    "train_df.loc[valid_train_mask, target_col] = target_scaler.fit_transform(train_df.loc[valid_train_mask, [target_col]])\n",
    "log_message(\"Data preprocessing complete.\")\n",
    "\n",
    "# =============================\n",
    "# 2. CUSTOM DATASET & DATALOADER\n",
    "# =============================\n",
    "log_message(\"Building custom dataset and dataloaders...\")\n",
    "\n",
    "class HydrologyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col=None):\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets  = df[target_col].values.astype(np.float32) if target_col in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Reduced batch size to help with memory issues\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = HydrologyDataset(train_df, feature_cols, target_col)\n",
    "test_dataset  = HydrologyDataset(test_df, feature_cols, target_col)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "log_message(\"Dataset and dataloaders ready.\")\n",
    "\n",
    "# =============================\n",
    "# 3. MODEL DEFINITION: PINN ARCHITECTURE\n",
    "# =============================\n",
    "log_message(\"Building model...\")\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PINNModel(in_features=len(feature_cols), out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "log_message(\"Model built and moved to device.\")\n",
    "\n",
    "# =============================\n",
    "# 4. LOSS FUNCTIONS & PHYSICS CONSTRAINTS\n",
    "# =============================\n",
    "log_message(\"Setting up loss functions...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def physics_loss(model, x, pred):\n",
    "    longitude_idx = 2  # Adjust if necessary\n",
    "    x.requires_grad_(True)\n",
    "    pred = model(x)\n",
    "    grad = torch.autograd.grad(outputs=pred, inputs=x, grad_outputs=torch.ones_like(pred),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    d_pred_d_long = grad[:, longitude_idx]\n",
    "    physics_residual = torch.mean(d_pred_d_long ** 2)\n",
    "    return physics_residual\n",
    "\n",
    "lambda_phys = 1.0\n",
    "\n",
    "def combined_loss(model, x, y_true):\n",
    "    y_pred = model(x)\n",
    "    valid_mask = ~torch.isnan(y_true)\n",
    "    if valid_mask.sum() > 0:\n",
    "        data_loss_val = mse_loss(y_pred[valid_mask], y_true[valid_mask])\n",
    "    else:\n",
    "        data_loss_val = 0.0 * y_pred.sum()  # maintain tensor type\n",
    "    phys_loss_val = physics_loss(model, x, y_pred)\n",
    "    total_loss = data_loss_val + lambda_phys * phys_loss_val\n",
    "    return total_loss, data_loss_val, phys_loss_val\n",
    "\n",
    "log_message(\"Loss functions set up.\")\n",
    "\n",
    "# =============================\n",
    "# 5. TRAINING SETUP: OPTIMIZERS, SCHEDULERS & LOGGING\n",
    "# =============================\n",
    "log_message(\"Setting up optimizer and logging...\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter(log_dir=\"runs/pinn_discharge_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_val_loss = float('inf')\n",
    "log_message(\"Training setup complete.\")\n",
    "\n",
    "# =============================\n",
    "# 6. TRAINING LOOP WITH RUNTIME MONITORING\n",
    "# =============================\n",
    "log_message(\"Starting training loop...\")\n",
    "start_time = time.time()\n",
    "\n",
    "n_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(n_epochs):\n",
    "        log_message(f\"Epoch {epoch+1}/{n_epochs} started.\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).to(device)  # Ensure shape (B, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, data_loss_val, phys_loss_val = combined_loss(model, x_batch, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_data_loss += data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val\n",
    "            epoch_phys_loss += phys_loss_val.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar('Train/Batch_Combined_Loss', loss.item(), global_step)\n",
    "                writer.add_scalar('Train/Batch_Data_Loss', data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val, global_step)\n",
    "                writer.add_scalar('Train/Batch_Physics_Loss', phys_loss_val.item(), global_step)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "        avg_phys_loss = epoch_phys_loss / len(train_loader)\n",
    "        log_message(f\"Epoch [{epoch+1}/{n_epochs}] complete. Loss: {avg_epoch_loss:.4f}, Data: {avg_data_loss:.4f}, Physics: {avg_phys_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar('Train/Epoch_Combined_Loss', avg_epoch_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Data_Loss', avg_data_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Physics_Loss', avg_phys_loss, epoch)\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.unsqueeze(1).to(device)\n",
    "                loss, _, _ = combined_loss(model, x_val, y_val)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        writer.add_scalar('Validation/Epoch_Loss', avg_val_loss, epoch)\n",
    "        log_message(f\"Validation Loss for Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"pinn_epoch{epoch+1:03d}_valloss{avg_val_loss:.4f}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            log_message(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Monitor GPU memory usage (if available)\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "            log_message(f\"GPU Memory - Allocated: {allocated:.1f} MB, Reserved: {reserved:.1f} MB\")\n",
    "except Exception as e:\n",
    "    log_message(\"An error occurred during training:\")\n",
    "    log_message(str(e))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "log_message(f\"Total runtime: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "\n",
    "# =============================\n",
    "# 7. OPTIONAL L-BFGS REFINEMENT\n",
    "# =============================\n",
    "# Uncomment and modify if L-BFGS refinement is desired.\n",
    "# def closure():\n",
    "#     optimizer_lbfgs.zero_grad()\n",
    "#     loss, _, _ = combined_loss(model, x_batch, y_batch)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "#\n",
    "# optimizer_lbfgs = optim.LBFGS(model.parameters(), lr=1e-2, max_iter=500)\n",
    "# model.train()\n",
    "# for epoch in range(5):  # a few epochs for refinement\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.unsqueeze(1).to(device)\n",
    "#         optimizer_lbfgs.step(closure)\n",
    "#     log_message(f\"L-BFGS refinement epoch {epoch+1} complete.\")\n",
    "\n",
    "# =============================\n",
    "# 8. SAVE FINAL MODEL & CLOSE LOGGING\n",
    "# =============================\n",
    "final_model_path = os.path.join(checkpoint_dir, \"pinn_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "log_message(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fce066-13a2-4cd9-8989-360f5f251908",
   "metadata": {},
   "outputs": [],
   "source": [
    "the code above workes. the next cell is improved code that take advange the gpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3928740-324e-4df8-9a09-b496b4616ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 10:24:17] Enabled cuDNN benchmark for optimized GPU performance.\n",
      "[2025-02-18 10:24:17] Starting data loading...\n",
      "[2025-02-18 10:24:17] Loading training data...\n",
      "[2025-02-18 10:24:17] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2012.csv\n",
      "[2025-02-18 10:25:37] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2013.csv\n",
      "[2025-02-18 10:26:57] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2014.csv\n",
      "[2025-02-18 10:28:16] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2015.csv\n",
      "[2025-02-18 10:29:36] Loading testing data...\n",
      "[2025-02-18 10:29:36] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2016.csv\n",
      "[2025-02-18 10:30:58] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2018.csv\n",
      "[2025-02-18 10:32:20] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2019.csv\n",
      "[2025-02-18 10:33:40] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2020.csv\n",
      "[2025-02-18 10:34:58] Data loading complete.\n",
      "[2025-02-18 10:34:58] Starting time feature encoding...\n",
      "[2025-02-18 10:34:58] Encoding training data time features...\n",
      "[2025-02-18 10:35:08] Encoding testing data time features...\n",
      "[2025-02-18 10:35:18] Starting feature scaling...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =============================\n",
    "# Logging Helper Function\n",
    "# =============================\n",
    "def log_message(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "# =============================\n",
    "# GPU Optimization Settings\n",
    "# =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True  # Use cuDNN auto-tuner to find best algorithm\n",
    "    log_message(\"Enabled cuDNN benchmark for optimized GPU performance.\")\n",
    "else:\n",
    "    log_message(\"Running on CPU.\")\n",
    "\n",
    "# =============================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# =============================\n",
    "log_message(\"Starting data loading...\")\n",
    "\n",
    "TRAIN_YEARS = ['2012', '2013', '2014', '2015']\n",
    "TEST_YEARS  = ['2016', '2018', '2019', '2020']\n",
    "\n",
    "DATA_DIR = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\"\n",
    "\n",
    "# Specify data types to reduce memory usage.\n",
    "dtypes = {\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'rainrate': np.float32,\n",
    "    'elevation': np.float32,\n",
    "    'slope_calculated': np.float32\n",
    "    # Add other columns if necessary.\n",
    "}\n",
    "\n",
    "def load_data(years):\n",
    "    df_list = []\n",
    "    for yr in years:\n",
    "        pattern = os.path.join(DATA_DIR, f\"*_{yr}.csv\")\n",
    "        for file in glob.glob(pattern):\n",
    "            log_message(f\"Loading file: {file}\")\n",
    "            df = pd.read_csv(file, parse_dates=['time'], dtype=dtypes)\n",
    "            df_list.append(df)\n",
    "    data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "log_message(\"Loading training data...\")\n",
    "train_df = load_data(TRAIN_YEARS)\n",
    "log_message(\"Loading testing data...\")\n",
    "test_df  = load_data(TEST_YEARS)\n",
    "log_message(\"Data loading complete.\")\n",
    "\n",
    "# Optionally, for debugging, you can use a smaller subset.\n",
    "# log_message(\"Using a small subset of data for debugging.\")\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "log_message(\"Starting time feature encoding...\")\n",
    "\n",
    "time_col     = 'time'\n",
    "spatial_cols = ['longitude', 'latitude', 'elevation', 'slope_calculated']\n",
    "hydro_cols   = ['rainrate']\n",
    "target_col   = 'rg_qms'\n",
    "\n",
    "def encode_time(df):\n",
    "    df['hour'] = df['time'].dt.hour + df['time'].dt.minute / 60.0 + df['time'].dt.second / 3600.0\n",
    "    df['time_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    return df\n",
    "\n",
    "log_message(\"Encoding training data time features...\")\n",
    "train_df = encode_time(train_df)\n",
    "log_message(\"Encoding testing data time features...\")\n",
    "test_df  = encode_time(test_df)\n",
    "\n",
    "feature_cols = ['time_sin', 'time_cos'] + spatial_cols + hydro_cols\n",
    "\n",
    "log_message(\"Starting feature scaling...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df[feature_cols]  = scaler.transform(test_df[feature_cols])\n",
    "log_message(\"Feature scaling complete.\")\n",
    "\n",
    "log_message(\"Scaling target values for valid measurements...\")\n",
    "target_scaler = StandardScaler()\n",
    "valid_train_mask = train_df[target_col].notna()\n",
    "train_df.loc[valid_train_mask, target_col] = target_scaler.fit_transform(train_df.loc[valid_train_mask, [target_col]])\n",
    "log_message(\"Data preprocessing complete.\")\n",
    "\n",
    "# =============================\n",
    "# 2. CUSTOM DATASET & DATALOADER\n",
    "# =============================\n",
    "log_message(\"Building custom dataset and dataloaders...\")\n",
    "\n",
    "class HydrologyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col=None):\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets  = df[target_col].values.astype(np.float32) if target_col in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Use a reduced batch size to help with memory issues.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# If using GPU, set pin_memory=True for faster host-to-device transfers.\n",
    "train_loader = DataLoader(\n",
    "    HydrologyDataset(train_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    HydrologyDataset(test_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "log_message(\"Dataset and dataloaders ready.\")\n",
    "\n",
    "# =============================\n",
    "# 3. MODEL DEFINITION: PINN ARCHITECTURE\n",
    "# =============================\n",
    "log_message(\"Building model...\")\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = PINNModel(in_features=len(feature_cols), out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "log_message(\"Model built and moved to device.\")\n",
    "\n",
    "# =============================\n",
    "# 4. LOSS FUNCTIONS & PHYSICS CONSTRAINTS\n",
    "# =============================\n",
    "log_message(\"Setting up loss functions...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def physics_loss(model, x, pred):\n",
    "    longitude_idx = 2  # Adjust if necessary\n",
    "    x.requires_grad_(True)\n",
    "    pred = model(x)\n",
    "    grad = torch.autograd.grad(outputs=pred, inputs=x, grad_outputs=torch.ones_like(pred),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    d_pred_d_long = grad[:, longitude_idx]\n",
    "    physics_residual = torch.mean(d_pred_d_long ** 2)\n",
    "    return physics_residual\n",
    "\n",
    "lambda_phys = 1.0\n",
    "\n",
    "def combined_loss(model, x, y_true):\n",
    "    y_pred = model(x)\n",
    "    valid_mask = ~torch.isnan(y_true)\n",
    "    if valid_mask.sum() > 0:\n",
    "        data_loss_val = mse_loss(y_pred[valid_mask], y_true[valid_mask])\n",
    "    else:\n",
    "        data_loss_val = 0.0 * y_pred.sum()  # maintain tensor type\n",
    "    phys_loss_val = physics_loss(model, x, y_pred)\n",
    "    total_loss = data_loss_val + lambda_phys * phys_loss_val\n",
    "    return total_loss, data_loss_val, phys_loss_val\n",
    "\n",
    "log_message(\"Loss functions set up.\")\n",
    "\n",
    "# =============================\n",
    "# 5. TRAINING SETUP: OPTIMIZER, CHECKPOINT RESUME & LOGGING\n",
    "# =============================\n",
    "log_message(\"Setting up optimizer and logging...\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter(log_dir=\"runs/pinn_discharge_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Resume training if a checkpoint exists.\n",
    "resume_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(resume_checkpoint):\n",
    "    checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    log_message(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    log_message(\"Starting training from scratch.\")\n",
    "\n",
    "# =============================\n",
    "# 6. TRAINING LOOP WITH RUNTIME MONITORING, CHECKPOINT SAVING & EPOCH TIME ESTIMATION\n",
    "# =============================\n",
    "log_message(\"Starting training loop...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "n_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        log_message(f\"Epoch {epoch+1}/{n_epochs} started.\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).to(device)  # Ensure shape (B, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, data_loss_val, phys_loss_val = combined_loss(model, x_batch, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_data_loss += data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val\n",
    "            epoch_phys_loss += phys_loss_val.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar('Train/Batch_Combined_Loss', loss.item(), global_step)\n",
    "                writer.add_scalar('Train/Batch_Data_Loss', data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val, global_step)\n",
    "                writer.add_scalar('Train/Batch_Physics_Loss', phys_loss_val.item(), global_step)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "        avg_phys_loss = epoch_phys_loss / len(train_loader)\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        est_remaining = (n_epochs - (epoch+1)) * epoch_duration\n",
    "\n",
    "        log_message(f\"Epoch [{epoch+1}/{n_epochs}] complete. Loss: {avg_epoch_loss:.4f}, Data: {avg_data_loss:.4f}, Physics: {avg_phys_loss:.4f}\")\n",
    "        log_message(f\"Epoch duration: {epoch_duration:.2f}s. Estimated remaining time: {est_remaining/60:.2f} minutes.\")\n",
    "\n",
    "        writer.add_scalar('Train/Epoch_Combined_Loss', avg_epoch_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Data_Loss', avg_data_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Physics_Loss', avg_phys_loss, epoch)\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.unsqueeze(1).to(device)\n",
    "                loss, _, _ = combined_loss(model, x_val, y_val)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        writer.add_scalar('Validation/Epoch_Loss', avg_val_loss, epoch)\n",
    "        log_message(f\"Validation Loss for Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if validation loss improved.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"pinn_epoch{epoch+1:03d}_valloss{avg_val_loss:.4f}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            log_message(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save latest checkpoint for resume.\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "\n",
    "        # Monitor GPU memory usage (if available)\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "            log_message(f\"GPU Memory - Allocated: {allocated:.1f} MB, Reserved: {reserved:.1f} MB\")\n",
    "except Exception as e:\n",
    "    log_message(\"An error occurred during training:\")\n",
    "    log_message(str(e))\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_elapsed = total_end_time - total_start_time\n",
    "hours, rem = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "log_message(f\"Total training runtime: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "\n",
    "# =============================\n",
    "# 7. OPTIONAL L-BFGS REFINEMENT (COMMENTED OUT)\n",
    "# =============================\n",
    "# def closure():\n",
    "#     optimizer_lbfgs.zero_grad()\n",
    "#     loss, _, _ = combined_loss(model, x_batch, y_batch)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "#\n",
    "# optimizer_lbfgs = optim.LBFGS(model.parameters(), lr=1e-2, max_iter=500)\n",
    "# model.train()\n",
    "# for epoch in range(5):  # a few epochs for refinement\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.unsqueeze(1).to(device)\n",
    "#         optimizer_lbfgs.step(closure)\n",
    "#     log_message(f\"L-BFGS refinement epoch {epoch+1} complete.\")\n",
    "\n",
    "# =============================\n",
    "# 8. SAVE FINAL MODEL & CLOSE LOGGING\n",
    "# =============================\n",
    "final_model_path = os.path.join(checkpoint_dir, \"pinn_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "log_message(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126a709-91ba-4aef-96a5-a3925e526fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "the last one is dies - also I figure out that the loss is without kynematic wave. so here is an update one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51a3e13-c62e-46b7-80ba-23171031e86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 10:41:19.589834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 10:41:21] Enabled cuDNN benchmark for optimized GPU performance.\n",
      "[2025-02-18 10:41:21] Starting data loading...\n",
      "[2025-02-18 10:41:21] Loading training data...\n",
      "[2025-02-18 10:41:21] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2012.csv\n",
      "[2025-02-18 10:42:43] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2013.csv\n",
      "[2025-02-18 10:44:02] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2014.csv\n",
      "[2025-02-18 10:45:21] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2015.csv\n",
      "[2025-02-18 10:46:39] Loading testing data...\n",
      "[2025-02-18 10:46:39] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2016.csv\n",
      "[2025-02-18 10:47:56] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2018.csv\n",
      "[2025-02-18 10:49:15] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2019.csv\n",
      "[2025-02-18 10:50:35] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2020.csv\n",
      "[2025-02-18 10:51:54] Data loading complete.\n",
      "[2025-02-18 10:51:54] Starting time feature encoding...\n",
      "[2025-02-18 10:51:54] Encoding training data time features...\n",
      "[2025-02-18 10:52:03] Encoding testing data time features...\n",
      "[2025-02-18 10:52:13] Starting feature scaling...\n",
      "[2025-02-18 10:53:09] Feature scaling complete.\n",
      "[2025-02-18 10:53:09] Scaling target values for valid measurements...\n",
      "[2025-02-18 10:53:10] Data preprocessing complete.\n",
      "[2025-02-18 10:53:10] Building custom dataset and dataloaders...\n",
      "[2025-02-18 10:53:18] Dataset and dataloaders ready.\n",
      "[2025-02-18 10:53:18] Building model...\n",
      "[2025-02-18 10:53:19] Model built and moved to device.\n",
      "[2025-02-18 10:53:19] Setting up loss functions...\n",
      "[2025-02-18 10:53:19] Loss functions set up.\n",
      "[2025-02-18 10:53:19] Setting up optimizer and logging...\n",
      "[2025-02-18 10:53:19] Starting training from scratch.\n",
      "[2025-02-18 10:53:19] Starting training loop...\n",
      "[2025-02-18 10:53:19] Epoch 1/50 started.\n",
      "[2025-02-18 15:54:15] Epoch [1/50] complete. Loss: nan, Data: nan, Physics: nan\n",
      "[2025-02-18 15:54:15] Epoch duration: 18056.04s. Estimated remaining time: 14745.76 minutes.\n",
      "[2025-02-18 15:54:15] An error occurred during training:\n",
      "[2025-02-18 15:54:15] element 0 of tensors does not require grad and does not have a grad_fn\n",
      "[2025-02-18 15:54:15] Total training runtime: 5h 0m 56.25s\n",
      "[2025-02-18 15:54:15] Final model saved to ./checkpoints/pinn_final.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =============================\n",
    "# ×¤×•× ×§×¦×™×™×ª ×¨×™×©×•× ×”×•×“×¢×•×ª (Logging)\n",
    "# =============================\n",
    "def log_message(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "# =============================\n",
    "# ×”×’×“×¨×•×ª ×œ××•×¤×˜×™×ž×™×–×¦×™×” ×©×œ GPU\n",
    "# =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True  # ×ž××¤×©×¨ ××•×˜×•×˜×™×•× ×™× ×’ ×©×œ cuDNN ×œ×‘×™×¦×•×¢×™× ×ž×™×˜×‘×™×™×\n",
    "    log_message(\"Enabled cuDNN benchmark for optimized GPU performance.\")\n",
    "else:\n",
    "    log_message(\"Running on CPU.\")\n",
    "\n",
    "# =============================\n",
    "# 1. ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ×•×”×›× ×”\n",
    "# =============================\n",
    "log_message(\"Starting data loading...\")\n",
    "\n",
    "TRAIN_YEARS = ['2012', '2013', '2014', '2015']\n",
    "TEST_YEARS  = ['2016', '2018', '2019', '2020']\n",
    "\n",
    "DATA_DIR = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\"\n",
    "\n",
    "# ×›×“×™ ×œ×—×¡×•×š ×‘×–×™×›×¨×•×Ÿ, × ×’×“×™×¨ ×¡×•×’×™ × ×ª×•× ×™×\n",
    "dtypes = {\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'rainrate': np.float32,\n",
    "    'elevation': np.float32,\n",
    "    'slope_calculated': np.float32\n",
    "    # ××¤×©×¨ ×œ×”×•×¡×™×£ ×¢×ž×•×“×•×ª × ×•×¡×¤×•×ª ×œ×¤×™ ×”×¦×•×¨×š.\n",
    "}\n",
    "\n",
    "def load_data(years):\n",
    "    df_list = []\n",
    "    for yr in years:\n",
    "        pattern = os.path.join(DATA_DIR, f\"*_{yr}.csv\")\n",
    "        for file in glob.glob(pattern):\n",
    "            log_message(f\"Loading file: {file}\")\n",
    "            df = pd.read_csv(file, parse_dates=['time'], dtype=dtypes)\n",
    "            df_list.append(df)\n",
    "    data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "log_message(\"Loading training data...\")\n",
    "train_df = load_data(TRAIN_YEARS)\n",
    "log_message(\"Loading testing data...\")\n",
    "test_df  = load_data(TEST_YEARS)\n",
    "log_message(\"Data loading complete.\")\n",
    "\n",
    "# ××¤×©×¨ ×œ×”×©×ª×ž×© ×‘×—×œ×§ ×§×˜×Ÿ ×ž×”× ×ª×•× ×™× ×œ×¦×•×¨×š ×“×™×‘×•×’ (×œ× ×—×•×‘×”)\n",
    "# log_message(\"Using a small subset of data for debugging.\")\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "log_message(\"Starting time feature encoding...\")\n",
    "\n",
    "# ×¢×™×‘×•×“ ×ª×›×•× ×ª ×”×–×ž×Ÿ:\n",
    "# ×ž×—×©×‘×™× ××ª ×”×©×¢×” ×›×¢×¨×š ×¢×©×¨×•× ×™ ×•×©×•×ž×¨×™× ××•×ª×” ×‘×¢×ž×•×“×” \"hour\"\n",
    "time_col = 'time'\n",
    "def encode_time(df):\n",
    "    df['hour'] = df['time'].dt.hour + df['time'].dt.minute / 60.0 + df['time'].dt.second / 3600.0\n",
    "    df['time_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    return df\n",
    "\n",
    "log_message(\"Encoding training data time features...\")\n",
    "train_df = encode_time(train_df)\n",
    "log_message(\"Encoding testing data time features...\")\n",
    "test_df  = encode_time(test_df)\n",
    "\n",
    "# ×›×¢×ª × ×‘× ×” ×¨×©×™×ž×ª ×ª×›×•× ×•×ª ×©×ž×›×™×œ×” ×’× ××ª \"hour\" ×œ×ž×˜×¨×ª ×—×™×©×•×‘ × ×’×–×¨×•×ª ×‘×–×ž×Ÿ\n",
    "feature_cols = ['hour', 'time_sin', 'time_cos'] + ['longitude', 'latitude', 'elevation', 'slope_calculated'] + ['rainrate']\n",
    "\n",
    "log_message(\"Starting feature scaling...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "log_message(\"Feature scaling complete.\")\n",
    "\n",
    "log_message(\"Scaling target values for valid measurements...\")\n",
    "target_col = 'rg_qms'\n",
    "target_scaler = StandardScaler()\n",
    "valid_train_mask = train_df[target_col].notna()\n",
    "train_df.loc[valid_train_mask, target_col] = target_scaler.fit_transform(train_df.loc[valid_train_mask, [target_col]])\n",
    "log_message(\"Data preprocessing complete.\")\n",
    "\n",
    "# =============================\n",
    "# 2. ×™×¦×™×¨×ª Dataset ×•×˜×•×¢×Ÿ × ×ª×•× ×™× (DataLoader)\n",
    "# =============================\n",
    "log_message(\"Building custom dataset and dataloaders...\")\n",
    "\n",
    "class HydrologyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col=None):\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets = df[target_col].values.astype(np.float32) if target_col in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    HydrologyDataset(train_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    HydrologyDataset(test_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "log_message(\"Dataset and dataloaders ready.\")\n",
    "\n",
    "# =============================\n",
    "# 3. ×”×’×“×¨×ª ×”×ž×•×“×œ (PINN ARCHITECTURE)\n",
    "# =============================\n",
    "log_message(\"Building model...\")\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = PINNModel(in_features=len(feature_cols), out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "log_message(\"Model built and moved to device.\")\n",
    "\n",
    "# =============================\n",
    "# 4. ×”×’×“×¨×ª ×¤×•× ×§×¦×™×•×ª ××•×‘×“×Ÿ ×•××™×œ×•×¦×™× ×¤×™×–×™×§×œ×™×™×\n",
    "# =============================\n",
    "log_message(\"Setting up loss functions...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# ×›××Ÿ ×× ×• ×ž×’×“×™×¨×™× ××ª \"××•×‘×“×Ÿ ×”×’×œ ×”×§×™× ×˜×ž×˜×™\":\n",
    "# ×× ×• ×ž×—×©×‘×™× ××ª ×”× ×’×–×¨×ª ×©×œ q (×”× ×™×‘×•×™) ×œ×¤×™ ×–×ž×Ÿ (×”×¢×ž×•×“×” 'hour') ×•×¤×™ x (×”×¢×ž×•×“×” 'longitude')\n",
    "# ×•×ž× ×¡×™× ×œ××›×•×£ ××ª ×”×ž×©×•×•××”: dq/dt + c * dq/dx = 0, ×›××©×¨ c ×§×‘×•×¢ (× × ×™×— c=1)\n",
    "def physics_loss(model, x):\n",
    "    # ×•×“× ×©-x ×ž××¤×©×¨ ×—×™×©×•×‘ × ×’×–×¨×•×ª\n",
    "    x.requires_grad_(True)\n",
    "    q = model(x)\n",
    "    # ×—×©×‘ ××ª ×”× ×’×–×¨×•×ª ×©×œ q ×‘×™×—×¡ ×œ×›×œ ×ª×›×•× ×”\n",
    "    grad = torch.autograd.grad(outputs=q, inputs=x, grad_outputs=torch.ones_like(q),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    # ×‘×”× ×—×” ×©×”×¢×ž×•×“×” 0 ×”×™× \"hour\" (×–×ž×Ÿ) ×•×”×¢×ž×•×“×” 3 ×”×™× \"longitude\"\n",
    "    dQ_dt = grad[:, 0]  # × ×’×–×¨×ª ×œ×¤×™ ×–×ž×Ÿ\n",
    "    dQ_dx = grad[:, 3]  # × ×’×–×¨×ª ×œ×¤×™ ×§×• ××•×¨×š\n",
    "    c = 1.0  # ×ž×”×™×¨×•×ª ×’×œ (×§×‘×•×¢)\n",
    "    residual = torch.mean((dQ_dt + c * dQ_dx)**2)\n",
    "    return residual\n",
    "\n",
    "lambda_phys = 1.0  # ×ž×©×§×œ ×œ××•×‘×“×Ÿ ×”×¤×™×–×™×§×œ×™\n",
    "\n",
    "def combined_loss(model, x, y_true):\n",
    "    # ×ž×—×©×‘ ××ª × ×™×‘×•×™ ×”×ž×•×“×œ q\n",
    "    y_pred = model(x)\n",
    "    # ×ž×—×©×‘ ××ª ××•×‘×“×Ÿ ×”× ×ª×•× ×™× (MSE) ×¨×§ ×¢×‘×•×¨ ×”×“×•×’×ž××•×ª ×‘×”×Ÿ ×™×© ×¢×¨×š ×ª×§×£\n",
    "    valid_mask = ~torch.isnan(y_true)\n",
    "    if valid_mask.sum() > 0:\n",
    "        data_loss_val = mse_loss(y_pred[valid_mask], y_true[valid_mask])\n",
    "    else:\n",
    "        data_loss_val = 0.0 * y_pred.sum()  # ×œ×©×ž×™×¨×ª ×˜×™×¤×•×¡ ×”×˜× ×–×•×¨\n",
    "    # ×ž×—×©×‘ ××ª ××•×‘×“×Ÿ ×”×’×œ ×”×§×™× ×˜×ž×˜×™\n",
    "    phys_loss_val = physics_loss(model, x)\n",
    "    total_loss = data_loss_val + lambda_phys * phys_loss_val\n",
    "    return total_loss, data_loss_val, phys_loss_val\n",
    "\n",
    "log_message(\"Loss functions set up.\")\n",
    "\n",
    "# =============================\n",
    "# 5. ×”×’×“×¨×ª ×”××•×¤×˜×™×ž×™×™×–×¨, ×©×ž×™×¨×ª ×”×ª×§×“×ž×•×ª (Checkpoint) ×•×¨×™×©×•×\n",
    "# =============================\n",
    "log_message(\"Setting up optimizer and logging...\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter(log_dir=\"runs/pinn_discharge_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ×‘×“×™×§×” ×”×× ×§×™×™×ž×ª × ×§×•×“×ª ×©×ž×™×¨×” ×œ×”×ž×©×š ××™×ž×•×Ÿ\n",
    "resume_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(resume_checkpoint):\n",
    "    checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    log_message(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    log_message(\"Starting training from scratch.\")\n",
    "\n",
    "# =============================\n",
    "# 6. ×œ×•×œ××ª ×”××™×ž×•×Ÿ ×¢× ×ž×“×™×“×ª ×–×ž×Ÿ ×¢×™×“×Ÿ ×•×©×ž×™×¨×ª × ×§×•×“×•×ª\n",
    "# =============================\n",
    "log_message(\"Starting training loop...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "n_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        log_message(f\"Epoch {epoch+1}/{n_epochs} started.\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).to(device)  # ×ž×‘×˜×™×— ×©×”×¦×•×¨×” ×ª×”×™×” (B, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, data_loss_val, phys_loss_val = combined_loss(model, x_batch, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_data_loss += data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val\n",
    "            epoch_phys_loss += phys_loss_val.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar('Train/Batch_Combined_Loss', loss.item(), global_step)\n",
    "                writer.add_scalar('Train/Batch_Data_Loss', data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val, global_step)\n",
    "                writer.add_scalar('Train/Batch_Physics_Loss', phys_loss_val.item(), global_step)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "        avg_phys_loss = epoch_phys_loss / len(train_loader)\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        est_remaining = (n_epochs - (epoch+1)) * epoch_duration\n",
    "\n",
    "        log_message(f\"Epoch [{epoch+1}/{n_epochs}] complete. Loss: {avg_epoch_loss:.4f}, Data: {avg_data_loss:.4f}, Physics: {avg_phys_loss:.4f}\")\n",
    "        log_message(f\"Epoch duration: {epoch_duration:.2f}s. Estimated remaining time: {est_remaining/60:.2f} minutes.\")\n",
    "\n",
    "        writer.add_scalar('Train/Epoch_Combined_Loss', avg_epoch_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Data_Loss', avg_data_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Physics_Loss', avg_phys_loss, epoch)\n",
    "\n",
    "        # ×©×œ×‘ ×‘×“×™×§×ª ×‘×™×¦×•×¢×™× (Validation)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.unsqueeze(1).to(device)\n",
    "                loss, _, _ = combined_loss(model, x_val, y_val)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        writer.add_scalar('Validation/Epoch_Loss', avg_val_loss, epoch)\n",
    "        log_message(f\"Validation Loss for Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ×©×ž×™×¨×ª × ×§×•×“×ª ×©×ž×™×¨×” ×× ×”××•×‘×“×Ÿ ×‘×‘×“×™×§×” ×”×©×ª×¤×¨\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"pinn_epoch{epoch+1:03d}_valloss{avg_val_loss:.4f}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            log_message(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # ×©×ž×™×¨×ª × ×§×•×“×ª ×©×ž×™×¨×” ×¢×“×›× ×™×ª (latest) ×œ×¦×•×¨×š ×”×ž×©×š ××™×ž×•×Ÿ ×‘×ž×™×“×” ×•×”××™×ž×•×Ÿ ×™×•×¤×¡×§\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "\n",
    "        # × ×™×˜×•×¨ ×©×™×ž×•×© ×‘×–×™×›×¨×•×Ÿ GPU (×× ×–×ž×™×Ÿ)\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "            log_message(f\"GPU Memory - Allocated: {allocated:.1f} MB, Reserved: {reserved:.1f} MB\")\n",
    "except Exception as e:\n",
    "    log_message(\"An error occurred during training:\")\n",
    "    log_message(str(e))\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_elapsed = total_end_time - total_start_time\n",
    "hours, rem = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "log_message(f\"Total training runtime: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "\n",
    "# =============================\n",
    "# 7. ××¤×©×¨×•×ª ×œ-L-BFGS (×ž×•×©×‘×ª)\n",
    "# =============================\n",
    "# def closure():\n",
    "#     optimizer_lbfgs.zero_grad()\n",
    "#     loss, _, _ = combined_loss(model, x_batch, y_batch)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "#\n",
    "# optimizer_lbfgs = optim.LBFGS(model.parameters(), lr=1e-2, max_iter=500)\n",
    "# model.train()\n",
    "# for epoch in range(5):  # ×›×ž×” ×¢×™×“× ×™× ×œ×œ×™×˜×•×©\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.unsqueeze(1).to(device)\n",
    "#         optimizer_lbfgs.step(closure)\n",
    "#     log_message(f\"L-BFGS refinement epoch {epoch+1} complete.\")\n",
    "\n",
    "# =============================\n",
    "# 8. ×©×ž×™×¨×ª ×”×ž×•×“×œ ×”×¡×•×¤×™ ×•×¡×’×™×¨×ª ×¨×™×©×•×\n",
    "# =============================\n",
    "final_model_path = os.path.join(checkpoint_dir, \"pinn_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "log_message(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33567f-197f-444d-a218-554a65c9995b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb52c62e-df5e-42da-aae1-abdf0f5aae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Define the Sine activation and the model architecture ---\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        # Hidden layers\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Set up the device and load the saved model checkpoint ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# In training, we used 8 features: ['hour', 'time_sin', 'time_cos', 'longitude', 'latitude', 'elevation', 'slope_calculated', 'rainrate']\n",
    "model = PINNModel(in_features=8, out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "checkpoint_path = \"./checkpoints/pinn_final.pth\"  # adjust if needed\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Loaded model from checkpoint.\")\n",
    "\n",
    "# --- Load and preprocess test data ---\n",
    "# For example, we load one of your test CSV files.\n",
    "# Ensure the CSV file contains the columns: time, longitude, latitude, elevation, slope_calculated, rainrate, and rg_qms.\n",
    "test_file = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2020.csv\"\n",
    "test_df = pd.read_csv(test_file, parse_dates=['time'])\n",
    "\n",
    "# Preprocess the time features as done in training:\n",
    "test_df[\"hour\"] = test_df[\"time\"].dt.hour + test_df[\"time\"].dt.minute/60.0 + test_df[\"time\"].dt.second/3600.0\n",
    "test_df[\"time_sin\"] = np.sin(2 * np.pi * test_df[\"hour\"] / 24.0)\n",
    "test_df[\"time_cos\"] = np.cos(2 * np.pi * test_df[\"hour\"] / 24.0)\n",
    "\n",
    "# Define feature columns (must match the training order)\n",
    "feature_cols = [\"hour\", \"time_sin\", \"time_cos\", \"longitude\", \"latitude\", \"elevation\", \"slope_calculated\", \"rainrate\"]\n",
    "\n",
    "# For visualization, we only consider rows where we have a valid target value:\n",
    "test_df_valid = test_df[test_df[\"rg_qms\"].notna()].copy()\n",
    "\n",
    "# Extract features and target:\n",
    "X_test = test_df_valid[feature_cols].values.astype(np.float32)\n",
    "y_true = test_df_valid[\"rg_qms\"].values.astype(np.float32)\n",
    "\n",
    "# Convert features to a torch tensor:\n",
    "X_test_tensor = torch.tensor(X_test).to(device)\n",
    "\n",
    "# --- Use the model to predict ---\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "\n",
    "# --- Visualization: Plot predicted vs. true discharge ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "plt.xlabel(\"True Discharge (normalized)\")\n",
    "plt.ylabel(\"Predicted Discharge (normalized)\")\n",
    "plt.title(\"Predicted vs True Discharge\")\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"r--\", label=\"Ideal\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3204cab5-81f8-4757-8996-3950ab6a8bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Define the Sine activation and the model architecture ---\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        # Hidden layers\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Set up the device and load the saved model checkpoint ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# We used 8 features during training: ['hour', 'time_sin', 'time_cos', 'longitude', 'latitude', 'elevation', 'slope_calculated', 'rainrate']\n",
    "model = PINNModel(in_features=8, out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "checkpoint_path = \"./checkpoints/pinn_final.pth\"  # Adjust path if needed\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Loaded model from checkpoint.\")\n",
    "\n",
    "# --- Load and preprocess test data ---\n",
    "# Change this to one of your test CSV files.\n",
    "test_file = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2020.csv\"\n",
    "test_df = pd.read_csv(test_file, parse_dates=['time'])\n",
    "\n",
    "# Preprocess the time features as done in training\n",
    "test_df[\"hour\"] = test_df[\"time\"].dt.hour + test_df[\"time\"].dt.minute/60.0 + test_df[\"time\"].dt.second/3600.0\n",
    "test_df[\"time_sin\"] = np.sin(2 * np.pi * test_df[\"hour\"] / 24.0)\n",
    "test_df[\"time_cos\"] = np.cos(2 * np.pi * test_df[\"hour\"] / 24.0)\n",
    "\n",
    "# Define feature columns in the same order as used during training.\n",
    "feature_cols = [\"hour\", \"time_sin\", \"time_cos\", \"longitude\", \"latitude\", \"elevation\", \"slope_calculated\", \"rainrate\"]\n",
    "\n",
    "# For visualization, we consider only rows where the target 'rg_qms' is valid.\n",
    "test_df_valid = test_df[test_df[\"rg_qms\"].notna()].copy()\n",
    "\n",
    "# Extract the features and the true target values\n",
    "X_test = test_df_valid[feature_cols].values.astype(np.float32)\n",
    "y_true = test_df_valid[\"rg_qms\"].values.astype(np.float32)\n",
    "\n",
    "# Convert features to a torch tensor.\n",
    "X_test_tensor = torch.tensor(X_test).to(device)\n",
    "\n",
    "# --- Compute predictions ---\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "\n",
    "# --- Create hydrograph plots on two separate subplots ---\n",
    "# Convert time column to datetime for plotting.\n",
    "times = pd.to_datetime(test_df_valid[\"time\"])\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Subplot 1: True discharge vs. Time\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(times, y_true, label=\"True Discharge\", color=\"blue\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"True Discharge (normalized)\")\n",
    "plt.title(\"Hydrograph - True Discharge\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Subplot 2: Predicted discharge vs. Time\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(times, y_pred, label=\"Predicted Discharge\", color=\"green\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Predicted Discharge (normalized)\")\n",
    "plt.title(\"Hydrograph - Predicted Discharge\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9d8908-ebec-4928-b443-ac74c86f4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test_tensor: torch.Size([118368, 8])\n",
      "Shape of y_pred_tensor: torch.Size([118368, 1])\n",
      "First 10 predictions: tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_test_tensor:\", X_test_tensor.shape)\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "print(\"Shape of y_pred_tensor:\", y_pred_tensor.shape)\n",
    "print(\"First 10 predictions:\", y_pred_tensor[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f280b8-3216-4240-8fe8-0e1a63170bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29d6d6-83e9-431b-b98b-7c013d73a367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8078db66-2687-4a43-94f5-29b9577704ab",
   "metadata": {},
   "source": [
    "training on 2012 and testing on 2013 so it will be easier to explore resault and looks for bugs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6ff19d-e0ec-4bf0-a26e-627e74567fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 07:02:48] Enabled cuDNN benchmark for optimized GPU performance.\n",
      "[2025-02-19 07:02:48] Starting data loading...\n",
      "[2025-02-19 07:02:48] Loading training data (2012)...\n",
      "[2025-02-19 07:02:48] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2012.csv\n",
      "[2025-02-19 07:04:09] Loading testing data (2013)...\n",
      "[2025-02-19 07:04:09] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2013.csv\n",
      "[2025-02-19 07:05:26] Data loading complete.\n",
      "[2025-02-19 07:05:26] Starting time feature encoding...\n",
      "[2025-02-19 07:05:26] Encoding training data time features...\n",
      "[2025-02-19 07:05:28] Encoding testing data time features...\n",
      "[2025-02-19 07:05:31] Starting feature scaling...\n",
      "[2025-02-19 07:05:37] Feature scaling complete.\n",
      "[2025-02-19 07:05:37] Scaling target values for valid measurements...\n",
      "[2025-02-19 07:05:37] Data preprocessing complete.\n",
      "[2025-02-19 07:05:37] Building custom dataset and dataloaders...\n",
      "[2025-02-19 07:05:39] Dataset and dataloaders ready.\n",
      "[2025-02-19 07:05:39] Building model...\n",
      "[2025-02-19 07:05:39] Model built and moved to device.\n",
      "[2025-02-19 07:05:39] Setting up loss functions...\n",
      "[2025-02-19 07:05:39] Loss functions set up.\n",
      "[2025-02-19 07:05:39] Setting up optimizer and logging...\n",
      "[2025-02-19 07:05:39] Starting training from scratch.\n",
      "[2025-02-19 07:05:39] Starting training loop...\n",
      "[2025-02-19 07:05:39] Epoch 1/50 started.\n",
      "[2025-02-19 07:05:41] Batch 0: Gradient norm = nan\n",
      "[2025-02-19 07:05:41] Batch 1: Gradient norm = nan\n",
      "[2025-02-19 07:05:41] Batch 2: Gradient norm = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 256\u001b[0m\n\u001b[1;32m    253\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    255\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 256\u001b[0m loss, data_loss_val, phys_loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Log gradient norms for the first few batches for diagnosis\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 201\u001b[0m, in \u001b[0;36mcombined_loss\u001b[0;34m(model, x, y_true)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     data_loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m*\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# preserve tensor type\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m phys_loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mphysics_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m data_loss_val \u001b[38;5;241m+\u001b[39m lambda_phys \u001b[38;5;241m*\u001b[39m phys_loss_val\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, data_loss_val, phys_loss_val\n",
      "Cell \u001b[0;32mIn[6], line 182\u001b[0m, in \u001b[0;36mphysics_loss\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mphysics_loss\u001b[39m(model, x):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Ensure x is a fresh leaf tensor that requires gradients.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 182\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs\u001b[38;5;241m=\u001b[39mq, inputs\u001b[38;5;241m=\u001b[39mx, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(q),\n\u001b[1;32m    184\u001b[0m                                create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Assume column 0 is \"hour\" (time) and column 3 is \"longitude\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 165\u001b[0m, in \u001b[0;36mPINNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =============================\n",
    "# Logging Helper Function\n",
    "# =============================\n",
    "def log_message(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "# =============================\n",
    "# GPU Optimization Settings\n",
    "# =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    log_message(\"Enabled cuDNN benchmark for optimized GPU performance.\")\n",
    "else:\n",
    "    log_message(\"Running on CPU.\")\n",
    "\n",
    "# =============================\n",
    "# 1. Data Loading & Preprocessing (Train on 2012, Test on 2013)\n",
    "# =============================\n",
    "log_message(\"Starting data loading...\")\n",
    "\n",
    "TRAIN_YEARS = ['2012']\n",
    "TEST_YEARS  = ['2013']\n",
    "\n",
    "DATA_DIR = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\"\n",
    "\n",
    "# Define data types to reduce memory usage.\n",
    "dtypes = {\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'rainrate': np.float32,\n",
    "    'elevation': np.float32,\n",
    "    'slope_calculated': np.float32\n",
    "}\n",
    "\n",
    "def load_data(years):\n",
    "    df_list = []\n",
    "    for yr in years:\n",
    "        pattern = os.path.join(DATA_DIR, f\"*_{yr}.csv\")\n",
    "        for file in glob.glob(pattern):\n",
    "            log_message(f\"Loading file: {file}\")\n",
    "            df = pd.read_csv(file, parse_dates=['time'], dtype=dtypes)\n",
    "            df_list.append(df)\n",
    "    data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "log_message(\"Loading training data (2012)...\")\n",
    "train_df = load_data(TRAIN_YEARS)\n",
    "log_message(\"Loading testing data (2013)...\")\n",
    "test_df  = load_data(TEST_YEARS)\n",
    "log_message(\"Data loading complete.\")\n",
    "\n",
    "# (Optional: for debugging, use a smaller subset)\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "log_message(\"Starting time feature encoding...\")\n",
    "\n",
    "def encode_time(df):\n",
    "    df['hour'] = df['time'].dt.hour + df['time'].dt.minute/60.0 + df['time'].dt.second/3600.0\n",
    "    df['time_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    return df\n",
    "\n",
    "log_message(\"Encoding training data time features...\")\n",
    "train_df = encode_time(train_df)\n",
    "log_message(\"Encoding testing data time features...\")\n",
    "test_df  = encode_time(test_df)\n",
    "\n",
    "# Build feature columns â€“ note we include 'hour' for gradient computation\n",
    "feature_cols = ['hour', 'time_sin', 'time_cos', 'longitude', 'latitude', 'elevation', 'slope_calculated', 'rainrate']\n",
    "\n",
    "log_message(\"Starting feature scaling...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "log_message(\"Feature scaling complete.\")\n",
    "\n",
    "log_message(\"Scaling target values for valid measurements...\")\n",
    "target_col = 'rg_qms'\n",
    "target_scaler = StandardScaler()\n",
    "valid_train_mask = train_df[target_col].notna()\n",
    "train_df.loc[valid_train_mask, target_col] = target_scaler.fit_transform(train_df.loc[valid_train_mask, [target_col]])\n",
    "log_message(\"Data preprocessing complete.\")\n",
    "\n",
    "# =============================\n",
    "# 2. Custom Dataset & DataLoader\n",
    "# =============================\n",
    "log_message(\"Building custom dataset and dataloaders...\")\n",
    "\n",
    "class HydrologyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col=None):\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets = df[target_col].values.astype(np.float32) if target_col in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    HydrologyDataset(train_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    HydrologyDataset(test_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "log_message(\"Dataset and dataloaders ready.\")\n",
    "\n",
    "# =============================\n",
    "# 3. Model Definition (PINN Architecture)\n",
    "# =============================\n",
    "log_message(\"Building model...\")\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = PINNModel(in_features=len(feature_cols), out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "log_message(\"Model built and moved to device.\")\n",
    "\n",
    "# =============================\n",
    "# 4. Loss Functions & Physics Constraints\n",
    "# =============================\n",
    "log_message(\"Setting up loss functions...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Physics Loss: Enforcing the kinematic wave equation: d(q)/dt + c*d(q)/dx = 0 (with c = 1)\n",
    "def physics_loss(model, x):\n",
    "    # Ensure x is a fresh leaf tensor that requires gradients.\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    q = model(x)\n",
    "    grad = torch.autograd.grad(outputs=q, inputs=x, grad_outputs=torch.ones_like(q),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    # Assume column 0 is \"hour\" (time) and column 3 is \"longitude\"\n",
    "    dQ_dt = grad[:, 0]\n",
    "    dQ_dx = grad[:, 3]\n",
    "    c = 1.0  # wave speed constant\n",
    "    residual = torch.mean((dQ_dt + c * dQ_dx) ** 2)\n",
    "    return residual\n",
    "\n",
    "lambda_phys = 0.1  # Lower the physics loss weight for stability\n",
    "\n",
    "def combined_loss(model, x, y_true):\n",
    "    y_pred = model(x)\n",
    "    valid_mask = ~torch.isnan(y_true)\n",
    "    if valid_mask.sum() > 0:\n",
    "        data_loss_val = mse_loss(y_pred[valid_mask], y_true[valid_mask])\n",
    "    else:\n",
    "        data_loss_val = 0.0 * y_pred.sum()  # preserve tensor type\n",
    "    phys_loss_val = physics_loss(model, x)\n",
    "    total_loss = data_loss_val + lambda_phys * phys_loss_val\n",
    "    return total_loss, data_loss_val, phys_loss_val\n",
    "\n",
    "log_message(\"Loss functions set up.\")\n",
    "\n",
    "# =============================\n",
    "# 5. Optimizer, Checkpointing, and Logging Setup\n",
    "# =============================\n",
    "log_message(\"Setting up optimizer and logging...\")\n",
    "\n",
    "# Lower learning rate for stability\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "writer = SummaryWriter(log_dir=\"runs/pinn_discharge_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Resume checkpoint if available\n",
    "resume_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(resume_checkpoint):\n",
    "    checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    log_message(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    log_message(\"Starting training from scratch.\")\n",
    "\n",
    "# =============================\n",
    "# 6. Training Loop with Epoch Timing, Checkpoint Saving & Gradient Norm Logging\n",
    "# =============================\n",
    "log_message(\"Starting training loop...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "n_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        log_message(f\"Epoch {epoch+1}/{n_epochs} started.\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, data_loss_val, phys_loss_val = combined_loss(model, x_batch, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Log gradient norms for the first few batches for diagnosis\n",
    "            if batch_idx < 3:\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "                log_message(f\"Batch {batch_idx}: Gradient norm = {total_norm:.4f}\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_data_loss += data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val\n",
    "            epoch_phys_loss += phys_loss_val.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar('Train/Batch_Combined_Loss', loss.item(), global_step)\n",
    "                writer.add_scalar('Train/Batch_Data_Loss', data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val, global_step)\n",
    "                writer.add_scalar('Train/Batch_Physics_Loss', phys_loss_val.item(), global_step)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "        avg_phys_loss = epoch_phys_loss / len(train_loader)\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        est_remaining = (n_epochs - (epoch+1)) * epoch_duration\n",
    "\n",
    "        log_message(f\"Epoch [{epoch+1}/{n_epochs}] complete. Loss: {avg_epoch_loss:.4f}, Data: {avg_data_loss:.4f}, Physics: {avg_phys_loss:.4f}\")\n",
    "        log_message(f\"Epoch duration: {epoch_duration:.2f}s. Estimated remaining time: {est_remaining/60:.2f} minutes.\")\n",
    "\n",
    "        writer.add_scalar('Train/Epoch_Combined_Loss', avg_epoch_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Data_Loss', avg_data_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Physics_Loss', avg_phys_loss, epoch)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.unsqueeze(1).to(device)\n",
    "                loss, _, _ = combined_loss(model, x_val, y_val)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        writer.add_scalar('Validation/Epoch_Loss', avg_val_loss, epoch)\n",
    "        log_message(f\"Validation Loss for Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if validation loss improves.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"pinn_epoch{epoch+1:03d}_valloss{avg_val_loss:.4f}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            log_message(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save the latest checkpoint for resume purposes.\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "\n",
    "        # Monitor GPU memory usage if available.\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "            log_message(f\"GPU Memory - Allocated: {allocated:.1f} MB, Reserved: {reserved:.1f} MB\")\n",
    "except Exception as e:\n",
    "    log_message(\"An error occurred during training:\")\n",
    "    log_message(str(e))\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_elapsed = total_end_time - total_start_time\n",
    "hours, rem = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "log_message(f\"Total training runtime: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "\n",
    "# =============================\n",
    "# Optional L-BFGS Refinement (Disabled)\n",
    "# =============================\n",
    "# def closure():\n",
    "#     optimizer_lbfgs.zero_grad()\n",
    "#     loss, _, _ = combined_loss(model, x_batch, y_batch)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "#\n",
    "# optimizer_lbfgs = optim.LBFGS(model.parameters(), lr=1e-2, max_iter=500)\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.unsqueeze(1).to(device)\n",
    "#         optimizer_lbfgs.step(closure)\n",
    "#     log_message(f\"L-BFGS refinement epoch {epoch+1} complete.\")\n",
    "\n",
    "# =============================\n",
    "# Final Model Saving and Closing Logging\n",
    "# =============================\n",
    "final_model_path = os.path.join(checkpoint_dir, \"pinn_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "log_message(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c48cd558-3fea-4216-8de0-12e8ef4ee4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 07:13:28] Enabled cuDNN benchmark for optimized GPU performance.\n",
      "[2025-02-19 07:13:28] Starting data loading...\n",
      "[2025-02-19 07:13:28] Loading training data (2012)...\n",
      "[2025-02-19 07:13:28] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2012.csv\n",
      "[2025-02-19 07:14:42] Loading testing data (2013)...\n",
      "[2025-02-19 07:14:42] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2013.csv\n",
      "[2025-02-19 07:16:00] Data loading complete.\n",
      "[2025-02-19 07:16:00] Starting time feature encoding...\n",
      "[2025-02-19 07:16:00] Encoding training data time features...\n",
      "[2025-02-19 07:16:02] Encoding testing data time features...\n",
      "[2025-02-19 07:16:05] Starting feature scaling...\n",
      "[2025-02-19 07:16:11] Feature scaling complete.\n",
      "[2025-02-19 07:16:11] Scaling target values for valid measurements...\n",
      "[2025-02-19 07:16:11] Data preprocessing complete.\n",
      "[2025-02-19 07:16:11] Building custom dataset and dataloaders...\n",
      "[2025-02-19 07:16:13] Dataset and dataloaders ready.\n",
      "[2025-02-19 07:16:13] Building model...\n",
      "[2025-02-19 07:16:13] Model built and moved to device.\n",
      "[2025-02-19 07:16:13] Setting up loss functions...\n",
      "[2025-02-19 07:16:13] Loss functions set up.\n",
      "[2025-02-19 07:16:13] Setting up optimizer and logging...\n",
      "[2025-02-19 07:16:13] Starting training from scratch.\n",
      "[2025-02-19 07:16:13] Starting training loop...\n",
      "[2025-02-19 07:16:13] Epoch 1/50 started.\n",
      "[2025-02-19 07:16:15] Batch 0: Gradient norm = nan\n",
      "[2025-02-19 07:16:15] Batch 1: Gradient norm = nan\n",
      "[2025-02-19 07:16:15] Batch 2: Gradient norm = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 256\u001b[0m\n\u001b[1;32m    253\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    255\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 256\u001b[0m loss, data_loss_val, phys_loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Log gradient norms for first few batches\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 202\u001b[0m, in \u001b[0;36mcombined_loss\u001b[0;34m(model, x, y_true)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     data_loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m*\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m--> 202\u001b[0m phys_loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mphysics_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m data_loss_val \u001b[38;5;241m+\u001b[39m lambda_phys \u001b[38;5;241m*\u001b[39m phys_loss_val\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, data_loss_val, phys_loss_val\n",
      "Cell \u001b[0;32mIn[7], line 183\u001b[0m, in \u001b[0;36mphysics_loss\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m    181\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    182\u001b[0m q \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m--> 183\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Assume: column 0 = 'hour' (time), column 3 = 'longitude'\u001b[39;00m\n\u001b[1;32m    186\u001b[0m dQ_dt \u001b[38;5;241m=\u001b[39m grad[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =============================\n",
    "# Logging Helper Function\n",
    "# =============================\n",
    "def log_message(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "# =============================\n",
    "# GPU Optimization Settings\n",
    "# =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True  # Enables cuDNN auto-tuner for optimal performance\n",
    "    log_message(\"Enabled cuDNN benchmark for optimized GPU performance.\")\n",
    "else:\n",
    "    log_message(\"Running on CPU.\")\n",
    "\n",
    "# =============================\n",
    "# 1. Data Loading & Preprocessing (Train on 2012, Test on 2013)\n",
    "# =============================\n",
    "log_message(\"Starting data loading...\")\n",
    "\n",
    "TRAIN_YEARS = ['2012']\n",
    "TEST_YEARS  = ['2013']\n",
    "\n",
    "DATA_DIR = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\"\n",
    "\n",
    "# Define data types to reduce memory usage.\n",
    "dtypes = {\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'rainrate': np.float32,\n",
    "    'elevation': np.float32,\n",
    "    'slope_calculated': np.float32\n",
    "}\n",
    "\n",
    "def load_data(years):\n",
    "    df_list = []\n",
    "    for yr in years:\n",
    "        pattern = os.path.join(DATA_DIR, f\"*_{yr}.csv\")\n",
    "        for file in glob.glob(pattern):\n",
    "            log_message(f\"Loading file: {file}\")\n",
    "            df = pd.read_csv(file, parse_dates=['time'], dtype=dtypes)\n",
    "            df_list.append(df)\n",
    "    data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "log_message(\"Loading training data (2012)...\")\n",
    "train_df = load_data(TRAIN_YEARS)\n",
    "log_message(\"Loading testing data (2013)...\")\n",
    "test_df  = load_data(TEST_YEARS)\n",
    "log_message(\"Data loading complete.\")\n",
    "\n",
    "# (Optional: for debugging, you can use a smaller subset)\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "log_message(\"Starting time feature encoding...\")\n",
    "\n",
    "def encode_time(df):\n",
    "    df['hour'] = df['time'].dt.hour + df['time'].dt.minute/60.0 + df['time'].dt.second/3600.0\n",
    "    df['time_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    return df\n",
    "\n",
    "log_message(\"Encoding training data time features...\")\n",
    "train_df = encode_time(train_df)\n",
    "log_message(\"Encoding testing data time features...\")\n",
    "test_df  = encode_time(test_df)\n",
    "\n",
    "# Build feature columns â€“ note we include 'hour' for gradient computation.\n",
    "feature_cols = ['hour', 'time_sin', 'time_cos'] + ['longitude', 'latitude', 'elevation', 'slope_calculated'] + ['rainrate']\n",
    "\n",
    "log_message(\"Starting feature scaling...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "log_message(\"Feature scaling complete.\")\n",
    "\n",
    "log_message(\"Scaling target values for valid measurements...\")\n",
    "target_col = 'rg_qms'\n",
    "target_scaler = StandardScaler()\n",
    "valid_train_mask = train_df[target_col].notna()\n",
    "train_df.loc[valid_train_mask, target_col] = target_scaler.fit_transform(train_df.loc[valid_train_mask, [target_col]])\n",
    "log_message(\"Data preprocessing complete.\")\n",
    "\n",
    "# =============================\n",
    "# 2. Custom Dataset & DataLoader\n",
    "# =============================\n",
    "log_message(\"Building custom dataset and dataloaders...\")\n",
    "\n",
    "class HydrologyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col=None):\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets = df[target_col].values.astype(np.float32) if target_col in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    HydrologyDataset(train_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    HydrologyDataset(test_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "log_message(\"Dataset and dataloaders ready.\")\n",
    "\n",
    "# =============================\n",
    "# 3. Model Definition (PINN Architecture)\n",
    "# =============================\n",
    "log_message(\"Building model...\")\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = PINNModel(in_features=len(feature_cols), out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "log_message(\"Model built and moved to device.\")\n",
    "\n",
    "# =============================\n",
    "# 4. Loss Functions & Physics Constraints\n",
    "# =============================\n",
    "log_message(\"Setting up loss functions...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Physics Loss: Enforce the kinematic wave equation: dq/dt + c*dq/dx = 0, where c=1.\n",
    "def physics_loss(model, x):\n",
    "    # Create a fresh copy of x that requires gradients\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    q = model(x)\n",
    "    grad = torch.autograd.grad(outputs=q, inputs=x, grad_outputs=torch.ones_like(q),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    # Assume: column 0 = 'hour' (time), column 3 = 'longitude'\n",
    "    dQ_dt = grad[:, 0]\n",
    "    dQ_dx = grad[:, 3]\n",
    "    c = 1.0\n",
    "    residual = torch.mean((dQ_dt + c * dQ_dx) ** 2)\n",
    "    return residual\n",
    "\n",
    "# Temporarily disable physics loss for stability by setting its weight to 0.\n",
    "lambda_phys = 0.0\n",
    "\n",
    "def combined_loss(model, x, y_true):\n",
    "    y_pred = model(x)\n",
    "    valid_mask = ~torch.isnan(y_true)\n",
    "    if valid_mask.sum() > 0:\n",
    "        data_loss_val = mse_loss(y_pred[valid_mask], y_true[valid_mask])\n",
    "    else:\n",
    "        data_loss_val = 0.0 * y_pred.sum()\n",
    "    phys_loss_val = physics_loss(model, x)\n",
    "    total_loss = data_loss_val + lambda_phys * phys_loss_val\n",
    "    return total_loss, data_loss_val, phys_loss_val\n",
    "\n",
    "log_message(\"Loss functions set up.\")\n",
    "\n",
    "# =============================\n",
    "# 5. Optimizer, Checkpointing, and Logging Setup\n",
    "# =============================\n",
    "log_message(\"Setting up optimizer and logging...\")\n",
    "\n",
    "# Lower the learning rate for stability.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "writer = SummaryWriter(log_dir=\"runs/pinn_discharge_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "resume_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(resume_checkpoint):\n",
    "    checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    log_message(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    log_message(\"Starting training from scratch.\")\n",
    "\n",
    "# =============================\n",
    "# 6. Training Loop with Epoch Timing, Checkpoint Saving & Gradient Norm Logging\n",
    "# =============================\n",
    "log_message(\"Starting training loop...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "n_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        log_message(f\"Epoch {epoch+1}/{n_epochs} started.\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, data_loss_val, phys_loss_val = combined_loss(model, x_batch, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Log gradient norms for first few batches\n",
    "            if batch_idx < 3:\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "                log_message(f\"Batch {batch_idx}: Gradient norm = {total_norm:.4f}\")\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_data_loss += data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val\n",
    "            epoch_phys_loss += phys_loss_val.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar('Train/Batch_Combined_Loss', loss.item(), global_step)\n",
    "                writer.add_scalar('Train/Batch_Data_Loss', data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val, global_step)\n",
    "                writer.add_scalar('Train/Batch_Physics_Loss', phys_loss_val.item(), global_step)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "        avg_phys_loss = epoch_phys_loss / len(train_loader)\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        est_remaining = (n_epochs - (epoch+1)) * epoch_duration\n",
    "\n",
    "        log_message(f\"Epoch [{epoch+1}/{n_epochs}] complete. Loss: {avg_epoch_loss:.4f}, Data: {avg_data_loss:.4f}, Physics: {avg_phys_loss:.4f}\")\n",
    "        log_message(f\"Epoch duration: {epoch_duration:.2f}s. Estimated remaining time: {est_remaining/60:.2f} minutes.\")\n",
    "\n",
    "        writer.add_scalar('Train/Epoch_Combined_Loss', avg_epoch_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Data_Loss', avg_data_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Physics_Loss', avg_phys_loss, epoch)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.unsqueeze(1).to(device)\n",
    "                loss, _, _ = combined_loss(model, x_val, y_val)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        writer.add_scalar('Validation/Epoch_Loss', avg_val_loss, epoch)\n",
    "        log_message(f\"Validation Loss for Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if validation loss improves.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"pinn_epoch{epoch+1:03d}_valloss{avg_val_loss:.4f}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            log_message(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save the latest checkpoint for resume purposes.\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "\n",
    "        # Monitor GPU memory usage if available.\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "            log_message(f\"GPU Memory - Allocated: {allocated:.1f} MB, Reserved: {reserved:.1f} MB\")\n",
    "except Exception as e:\n",
    "    log_message(\"An error occurred during training:\")\n",
    "    log_message(str(e))\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_elapsed = total_end_time - total_start_time\n",
    "hours, rem = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "log_message(f\"Total training runtime: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "\n",
    "# =============================\n",
    "# Optional L-BFGS Refinement (Disabled)\n",
    "# =============================\n",
    "# def closure():\n",
    "#     optimizer_lbfgs.zero_grad()\n",
    "#     loss, _, _ = combined_loss(model, x_batch, y_batch)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "#\n",
    "# optimizer_lbfgs = optim.LBFGS(model.parameters(), lr=1e-2, max_iter=500)\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.unsqueeze(1).to(device)\n",
    "#         optimizer_lbfgs.step(closure)\n",
    "#     log_message(f\"L-BFGS refinement epoch {epoch+1} complete.\")\n",
    "\n",
    "# =============================\n",
    "# Final Model Saving and Logging Close\n",
    "# =============================\n",
    "final_model_path = os.path.join(checkpoint_dir, \"pinn_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "log_message(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb38693d-2cf0-4106-93f4-68b3ce967503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 07:19:21] Enabled cuDNN benchmark for optimized GPU performance.\n",
      "[2025-02-19 07:19:21] Starting data loading...\n",
      "[2025-02-19 07:19:21] Loading training data (2012)...\n",
      "[2025-02-19 07:19:21] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2012.csv\n",
      "[2025-02-19 07:20:37] Loading testing data (2013)...\n",
      "[2025-02-19 07:20:37] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2013.csv\n",
      "[2025-02-19 07:21:54] Data loading complete.\n",
      "[2025-02-19 07:21:54] Starting time feature encoding...\n",
      "[2025-02-19 07:21:54] Encoding training data time features...\n",
      "[2025-02-19 07:21:57] Encoding testing data time features...\n",
      "[2025-02-19 07:21:59] Starting feature scaling...\n",
      "[2025-02-19 07:22:06] Feature scaling complete.\n",
      "[2025-02-19 07:22:06] Scaling target values for valid measurements...\n",
      "[2025-02-19 07:22:06] Data preprocessing complete.\n",
      "[2025-02-19 07:22:06] Building custom dataset and dataloaders...\n",
      "[2025-02-19 07:22:08] Dataset and dataloaders ready.\n",
      "[2025-02-19 07:22:08] Building model...\n",
      "[2025-02-19 07:22:08] Model built and moved to device.\n",
      "[2025-02-19 07:22:08] Setting up loss functions...\n",
      "[2025-02-19 07:22:08] Loss functions set up.\n",
      "[2025-02-19 07:22:08] Setting up optimizer and logging...\n",
      "[2025-02-19 07:22:08] Starting training from scratch.\n",
      "[2025-02-19 07:22:08] Starting training loop...\n",
      "[2025-02-19 07:22:08] Epoch 1/50 started.\n",
      "[2025-02-19 07:22:10] Batch 0: Gradient norm = nan\n",
      "[2025-02-19 07:22:10] Batch 1: Gradient norm = nan\n",
      "[2025-02-19 07:22:10] Batch 2: Gradient norm = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 260\u001b[0m\n\u001b[1;32m    258\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    259\u001b[0m loss, data_loss_val, phys_loss_val \u001b[38;5;241m=\u001b[39m combined_loss(model, x_batch, y_batch)\n\u001b[0;32m--> 260\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Log gradient norms for first few batches for diagnosis\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =============================\n",
    "# Logging Helper Function\n",
    "# =============================\n",
    "def log_message(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "# =============================\n",
    "# GPU Optimization Settings\n",
    "# =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True  # Enables cuDNN auto-tuner for optimal performance\n",
    "    log_message(\"Enabled cuDNN benchmark for optimized GPU performance.\")\n",
    "else:\n",
    "    log_message(\"Running on CPU.\")\n",
    "\n",
    "# =============================\n",
    "# 1. Data Loading & Preprocessing (Train on 2012, Test on 2013)\n",
    "# =============================\n",
    "log_message(\"Starting data loading...\")\n",
    "\n",
    "TRAIN_YEARS = ['2012']\n",
    "TEST_YEARS  = ['2013']\n",
    "\n",
    "DATA_DIR = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\"\n",
    "\n",
    "# Define data types to reduce memory usage.\n",
    "dtypes = {\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'rainrate': np.float32,\n",
    "    'elevation': np.float32,\n",
    "    'slope_calculated': np.float32\n",
    "}\n",
    "\n",
    "def load_data(years):\n",
    "    df_list = []\n",
    "    for yr in years:\n",
    "        pattern = os.path.join(DATA_DIR, f\"*_{yr}.csv\")\n",
    "        for file in glob.glob(pattern):\n",
    "            log_message(f\"Loading file: {file}\")\n",
    "            df = pd.read_csv(file, parse_dates=['time'], dtype=dtypes)\n",
    "            df_list.append(df)\n",
    "    data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "log_message(\"Loading training data (2012)...\")\n",
    "train_df = load_data(TRAIN_YEARS)\n",
    "log_message(\"Loading testing data (2013)...\")\n",
    "test_df  = load_data(TEST_YEARS)\n",
    "log_message(\"Data loading complete.\")\n",
    "\n",
    "# (Optional: for debugging, you can use a smaller subset)\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "log_message(\"Starting time feature encoding...\")\n",
    "\n",
    "def encode_time(df):\n",
    "    df['hour'] = df['time'].dt.hour + df['time'].dt.minute/60.0 + df['time'].dt.second/3600.0\n",
    "    df['time_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    return df\n",
    "\n",
    "log_message(\"Encoding training data time features...\")\n",
    "train_df = encode_time(train_df)\n",
    "log_message(\"Encoding testing data time features...\")\n",
    "test_df  = encode_time(test_df)\n",
    "\n",
    "# Build feature columns â€“ include 'hour' for computing time derivatives\n",
    "feature_cols = ['hour', 'time_sin', 'time_cos'] + ['longitude', 'latitude', 'elevation', 'slope_calculated'] + ['rainrate']\n",
    "\n",
    "log_message(\"Starting feature scaling...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "log_message(\"Feature scaling complete.\")\n",
    "\n",
    "log_message(\"Scaling target values for valid measurements...\")\n",
    "target_col = 'rg_qms'\n",
    "target_scaler = StandardScaler()\n",
    "valid_train_mask = train_df[target_col].notna()\n",
    "train_df.loc[valid_train_mask, target_col] = target_scaler.fit_transform(train_df.loc[valid_train_mask, [target_col]])\n",
    "log_message(\"Data preprocessing complete.\")\n",
    "\n",
    "# =============================\n",
    "# 2. Custom Dataset & DataLoader\n",
    "# =============================\n",
    "log_message(\"Building custom dataset and dataloaders...\")\n",
    "\n",
    "class HydrologyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col=None):\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets = df[target_col].values.astype(np.float32) if target_col in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    HydrologyDataset(train_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    HydrologyDataset(test_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "log_message(\"Dataset and dataloaders ready.\")\n",
    "\n",
    "# =============================\n",
    "# 3. Model Definition (PINN Architecture)\n",
    "# =============================\n",
    "log_message(\"Building model...\")\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = PINNModel(in_features=len(feature_cols), out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "log_message(\"Model built and moved to device.\")\n",
    "\n",
    "# =============================\n",
    "# 4. Loss Functions & Physics Constraints\n",
    "# =============================\n",
    "log_message(\"Setting up loss functions...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Physics Loss: Enforce the kinematic wave equation: dq/dt + c*dq/dx = 0, with c=1.\n",
    "# We compute gradients with respect to 'hour' (assumed column 0) and 'longitude' (assumed column 3).\n",
    "def physics_loss(model, x):\n",
    "    # Create a fresh leaf tensor that requires gradients.\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    q = model(x)\n",
    "    grad = torch.autograd.grad(outputs=q, inputs=x, grad_outputs=torch.ones_like(q),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    dQ_dt = grad[:, 0]  # derivative with respect to time ('hour')\n",
    "    dQ_dx = grad[:, 3]  # derivative with respect to space ('longitude')\n",
    "    c = 1.0\n",
    "    residual = torch.mean((dQ_dt + c * dQ_dx) ** 2)\n",
    "    return residual\n",
    "\n",
    "# Temporarily disable physics loss to isolate problems.\n",
    "lambda_phys = 0.0\n",
    "\n",
    "def combined_loss(model, x, y_true):\n",
    "    y_pred = model(x)\n",
    "    valid_mask = ~torch.isnan(y_true)\n",
    "    if valid_mask.sum() > 0:\n",
    "        data_loss_val = mse_loss(y_pred[valid_mask], y_true[valid_mask])\n",
    "    else:\n",
    "        data_loss_val = 0.0 * y_pred.sum()\n",
    "    # Only add physics loss if lambda_phys is non-zero.\n",
    "    if lambda_phys != 0:\n",
    "        phys_loss_val = physics_loss(model, x)\n",
    "    else:\n",
    "        phys_loss_val = torch.tensor(0.0, device=device)\n",
    "    total_loss = data_loss_val + lambda_phys * phys_loss_val\n",
    "    return total_loss, data_loss_val, phys_loss_val\n",
    "\n",
    "log_message(\"Loss functions set up.\")\n",
    "\n",
    "# =============================\n",
    "# 5. Optimizer, Checkpointing, and Logging Setup\n",
    "# =============================\n",
    "log_message(\"Setting up optimizer and logging...\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "writer = SummaryWriter(log_dir=\"runs/pinn_discharge_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "resume_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(resume_checkpoint):\n",
    "    checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    log_message(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    log_message(\"Starting training from scratch.\")\n",
    "\n",
    "# =============================\n",
    "# 6. Training Loop with Epoch Timing, Checkpoint Saving & Gradient Norm Logging\n",
    "# =============================\n",
    "log_message(\"Starting training loop...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "n_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        log_message(f\"Epoch {epoch+1}/{n_epochs} started.\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, data_loss_val, phys_loss_val = combined_loss(model, x_batch, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Log gradient norms for first few batches for diagnosis\n",
    "            if batch_idx < 3:\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "                log_message(f\"Batch {batch_idx}: Gradient norm = {total_norm:.4f}\")\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_data_loss += data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val\n",
    "            epoch_phys_loss += phys_loss_val.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar('Train/Batch_Combined_Loss', loss.item(), global_step)\n",
    "                writer.add_scalar('Train/Batch_Data_Loss', data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val, global_step)\n",
    "                writer.add_scalar('Train/Batch_Physics_Loss', phys_loss_val.item(), global_step)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "        avg_phys_loss = epoch_phys_loss / len(train_loader)\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        est_remaining = (n_epochs - (epoch+1)) * epoch_duration\n",
    "\n",
    "        log_message(f\"Epoch [{epoch+1}/{n_epochs}] complete. Loss: {avg_epoch_loss:.4f}, Data: {avg_data_loss:.4f}, Physics: {avg_phys_loss:.4f}\")\n",
    "        log_message(f\"Epoch duration: {epoch_duration:.2f}s. Estimated remaining time: {est_remaining/60:.2f} minutes.\")\n",
    "\n",
    "        writer.add_scalar('Train/Epoch_Combined_Loss', avg_epoch_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Data_Loss', avg_data_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Physics_Loss', avg_phys_loss, epoch)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.unsqueeze(1).to(device)\n",
    "                loss, _, _ = combined_loss(model, x_val, y_val)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        writer.add_scalar('Validation/Epoch_Loss', avg_val_loss, epoch)\n",
    "        log_message(f\"Validation Loss for Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if validation loss improves.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"pinn_epoch{epoch+1:03d}_valloss{avg_val_loss:.4f}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            log_message(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save the latest checkpoint for resume purposes.\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "\n",
    "        # Monitor GPU memory usage if available.\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "            log_message(f\"GPU Memory - Allocated: {allocated:.1f} MB, Reserved: {reserved:.1f} MB\")\n",
    "except Exception as e:\n",
    "    log_message(\"An error occurred during training:\")\n",
    "    log_message(str(e))\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_elapsed = total_end_time - total_start_time\n",
    "hours, rem = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "log_message(f\"Total training runtime: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "\n",
    "# =============================\n",
    "# Optional L-BFGS Refinement (Disabled)\n",
    "# =============================\n",
    "# def closure():\n",
    "#     optimizer_lbfgs.zero_grad()\n",
    "#     loss, _, _ = combined_loss(model, x_batch, y_batch)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "#\n",
    "# optimizer_lbfgs = optim.LBFGS(model.parameters(), lr=1e-2, max_iter=500)\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.unsqueeze(1).to(device)\n",
    "#         optimizer_lbfgs.step(closure)\n",
    "#     log_message(f\"L-BFGS refinement epoch {epoch+1} complete.\")\n",
    "\n",
    "# =============================\n",
    "# Final Model Saving and Closing Logging\n",
    "# =============================\n",
    "final_model_path = os.path.join(checkpoint_dir, \"pinn_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "log_message(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0414050-88fc-476f-a346-b645052cc5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1e6255-7dfe-439c-a292-d9fdd5b37a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 07:24:53] Enabled cuDNN benchmark for optimized GPU performance.\n",
      "[2025-02-19 07:24:53] Starting data loading...\n",
      "[2025-02-19 07:24:53] Loading training data (2012)...\n",
      "[2025-02-19 07:24:53] Loading file: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/Merged_North_Dead_Sea_2012.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m     61\u001b[0m log_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading training data (2012)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_YEARS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m log_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading testing data (2013)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m test_df  \u001b[38;5;241m=\u001b[39m load_data(TEST_YEARS)\n",
      "Cell \u001b[0;32mIn[9], line 56\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(years)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(pattern):\n\u001b[1;32m     55\u001b[0m         log_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m         df_list\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     58\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(df_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[0;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/internals/construction.py:443\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[0;32m--> 443\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     missing \u001b[38;5;241m=\u001b[39m arrays\u001b[38;5;241m.\u001b[39misna()\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/series.py:537\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    535\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m--> 537\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/series.py:651\u001b[0m, in \u001b[0;36mSeries._init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    648\u001b[0m     keys, values \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;241m0\u001b[39m), []\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Input is now list-like, so rely on \"standard\" construction:\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Now we just make sure the order is respected, if any\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/construction.py:651\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    648\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_try_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_convert_platform(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/construction.py:793\u001b[0m, in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ndarray:\n\u001b[0;32m--> 793\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_1d_object_array_from_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m subarr\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(arr)\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1601\u001b[0m, in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;66;03m# numpy will try to interpret nested lists as further dimensions, hence\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;66;03m# making a 1D array that contains list-likes is a bit tricky:\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1601\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/series.py:1031\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1031\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1033\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:643\u001b[0m, in \u001b[0;36mDatetimeArray.__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz:\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;66;03m# The default for tz-aware is object, to preserve tz info\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:359\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: NpDtype \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# used for Timedelta/DatetimeArray, overwritten by PeriodArray\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m--> 359\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =============================\n",
    "# Logging Helper Function\n",
    "# =============================\n",
    "def log_message(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "# =============================\n",
    "# GPU Optimization Settings\n",
    "# =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True  # Enables cuDNN auto-tuner for optimal performance\n",
    "    log_message(\"Enabled cuDNN benchmark for optimized GPU performance.\")\n",
    "else:\n",
    "    log_message(\"Running on CPU.\")\n",
    "\n",
    "# =============================\n",
    "# 1. Data Loading & Preprocessing (Train on 2012, Test on 2013)\n",
    "#     Only use December, January, and February data.\n",
    "# =============================\n",
    "log_message(\"Starting data loading...\")\n",
    "\n",
    "TRAIN_YEARS = ['2012']\n",
    "TEST_YEARS  = ['2013']\n",
    "\n",
    "DATA_DIR = \"/media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/\"\n",
    "\n",
    "# Define data types to reduce memory usage.\n",
    "dtypes = {\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'rainrate': np.float32,\n",
    "    'elevation': np.float32,\n",
    "    'slope_calculated': np.float32\n",
    "}\n",
    "\n",
    "def load_data(years):\n",
    "    df_list = []\n",
    "    for yr in years:\n",
    "        pattern = os.path.join(DATA_DIR, f\"*_{yr}.csv\")\n",
    "        for file in glob.glob(pattern):\n",
    "            log_message(f\"Loading file: {file}\")\n",
    "            df = pd.read_csv(file, parse_dates=['time'], dtype=dtypes)\n",
    "            df_list.append(df)\n",
    "    data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "log_message(\"Loading training data (2012)...\")\n",
    "train_df = load_data(TRAIN_YEARS)\n",
    "log_message(\"Loading testing data (2013)...\")\n",
    "test_df  = load_data(TEST_YEARS)\n",
    "\n",
    "# Filter data for only December (12), January (1), and February (2)\n",
    "train_df = train_df[train_df['time'].dt.month.isin([12, 1, 2])]\n",
    "test_df  = test_df[test_df['time'].dt.month.isin([12, 1, 2])]\n",
    "\n",
    "log_message(\"Data loading complete.\")\n",
    "\n",
    "# (Optional: for debugging, use a smaller subset)\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "log_message(\"Starting time feature encoding...\")\n",
    "\n",
    "def encode_time(df):\n",
    "    df['hour'] = df['time'].dt.hour + df['time'].dt.minute/60.0 + df['time'].dt.second/3600.0\n",
    "    df['time_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    return df\n",
    "\n",
    "log_message(\"Encoding training data time features...\")\n",
    "train_df = encode_time(train_df)\n",
    "log_message(\"Encoding testing data time features...\")\n",
    "test_df  = encode_time(test_df)\n",
    "\n",
    "# Build feature columns â€“ note we include 'hour' for gradient computations.\n",
    "feature_cols = ['hour', 'time_sin', 'time_cos'] + ['longitude', 'latitude', 'elevation', 'slope_calculated'] + ['rainrate']\n",
    "\n",
    "log_message(\"Starting feature scaling...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "log_message(\"Feature scaling complete.\")\n",
    "\n",
    "log_message(\"Scaling target values for valid measurements...\")\n",
    "target_col = 'rg_qms'\n",
    "target_scaler = StandardScaler()\n",
    "valid_train_mask = train_df[target_col].notna()\n",
    "train_df.loc[valid_train_mask, target_col] = target_scaler.fit_transform(train_df.loc[valid_train_mask, [target_col]])\n",
    "log_message(\"Data preprocessing complete.\")\n",
    "\n",
    "# =============================\n",
    "# 2. Custom Dataset & DataLoader\n",
    "# =============================\n",
    "log_message(\"Building custom dataset and dataloaders...\")\n",
    "\n",
    "class HydrologyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col=None):\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets = df[target_col].values.astype(np.float32) if target_col in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    HydrologyDataset(train_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    HydrologyDataset(test_df, feature_cols, target_col),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "log_message(\"Dataset and dataloaders ready.\")\n",
    "\n",
    "# =============================\n",
    "# 3. Model Definition (PINN Architecture)\n",
    "# =============================\n",
    "log_message(\"Building model...\")\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layers=5, neurons=128, dropout_prob=0.1):\n",
    "        super(PINNModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, neurons))\n",
    "        layers.append(Sine())\n",
    "        layers.append(nn.BatchNorm1d(neurons))\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons, neurons))\n",
    "            layers.append(Sine())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(neurons, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = PINNModel(in_features=len(feature_cols), out_features=1, hidden_layers=5, neurons=128, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "log_message(\"Model built and moved to device.\")\n",
    "\n",
    "# =============================\n",
    "# 4. Loss Functions & Physics Constraints\n",
    "# =============================\n",
    "log_message(\"Setting up loss functions...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Physics Loss: Enforce the kinematic wave equation: dq/dt + c*dq/dx = 0 (with c = 1).\n",
    "# We compute gradients with respect to 'hour' (assumed column 0) and 'longitude' (assumed column 3).\n",
    "def physics_loss(model, x):\n",
    "    # Create a fresh copy of x that requires gradients.\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    q = model(x)\n",
    "    grad = torch.autograd.grad(outputs=q, inputs=x, grad_outputs=torch.ones_like(q),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    dQ_dt = grad[:, 0]  # derivative with respect to time (\"hour\")\n",
    "    dQ_dx = grad[:, 3]  # derivative with respect to space (\"longitude\")\n",
    "    c = 1.0\n",
    "    residual = torch.mean((dQ_dt + c * dQ_dx) ** 2)\n",
    "    return residual\n",
    "\n",
    "# Temporarily disable physics loss for stability.\n",
    "lambda_phys = 0.0\n",
    "\n",
    "def combined_loss(model, x, y_true):\n",
    "    y_pred = model(x)\n",
    "    valid_mask = ~torch.isnan(y_true)\n",
    "    if valid_mask.sum() > 0:\n",
    "        data_loss_val = mse_loss(y_pred[valid_mask], y_true[valid_mask])\n",
    "    else:\n",
    "        data_loss_val = 0.0 * y_pred.sum()\n",
    "    if lambda_phys != 0:\n",
    "        phys_loss_val = physics_loss(model, x)\n",
    "    else:\n",
    "        phys_loss_val = torch.tensor(0.0, device=device)\n",
    "    total_loss = data_loss_val + lambda_phys * phys_loss_val\n",
    "    return total_loss, data_loss_val, phys_loss_val\n",
    "\n",
    "log_message(\"Loss functions set up.\")\n",
    "\n",
    "# =============================\n",
    "# 5. Optimizer, Checkpointing, and Logging Setup\n",
    "# =============================\n",
    "log_message(\"Setting up optimizer and logging...\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "writer = SummaryWriter(log_dir=\"runs/pinn_discharge_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "resume_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(resume_checkpoint):\n",
    "    checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    log_message(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    log_message(\"Starting training from scratch.\")\n",
    "\n",
    "# =============================\n",
    "# 6. Training Loop with Epoch Timing, Checkpoint Saving & Gradient Norm Logging\n",
    "# =============================\n",
    "log_message(\"Starting training loop...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "n_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        log_message(f\"Epoch {epoch+1}/{n_epochs} started.\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_data_loss = 0.0\n",
    "        epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, data_loss_val, phys_loss_val = combined_loss(model, x_batch, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Log gradient norms for the first few batches for diagnosis.\n",
    "            if batch_idx < 3:\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "                log_message(f\"Batch {batch_idx}: Gradient norm = {total_norm:.4f}\")\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_data_loss += data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val\n",
    "            epoch_phys_loss += phys_loss_val.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar('Train/Batch_Combined_Loss', loss.item(), global_step)\n",
    "                writer.add_scalar('Train/Batch_Data_Loss', data_loss_val.item() if isinstance(data_loss_val, torch.Tensor) else data_loss_val, global_step)\n",
    "                writer.add_scalar('Train/Batch_Physics_Loss', phys_loss_val.item(), global_step)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "        avg_phys_loss = epoch_phys_loss / len(train_loader)\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        est_remaining = (n_epochs - (epoch+1)) * epoch_duration\n",
    "\n",
    "        log_message(f\"Epoch [{epoch+1}/{n_epochs}] complete. Loss: {avg_epoch_loss:.4f}, Data: {avg_data_loss:.4f}, Physics: {avg_phys_loss:.4f}\")\n",
    "        log_message(f\"Epoch duration: {epoch_duration:.2f}s. Estimated remaining time: {est_remaining/60:.2f} minutes.\")\n",
    "\n",
    "        writer.add_scalar('Train/Epoch_Combined_Loss', avg_epoch_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Data_Loss', avg_data_loss, epoch)\n",
    "        writer.add_scalar('Train/Epoch_Physics_Loss', avg_phys_loss, epoch)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.unsqueeze(1).to(device)\n",
    "                loss, _, _ = combined_loss(model, x_val, y_val)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        writer.add_scalar('Validation/Epoch_Loss', avg_val_loss, epoch)\n",
    "        log_message(f\"Validation Loss for Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if validation loss improves.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"pinn_epoch{epoch+1:03d}_valloss{avg_val_loss:.4f}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            log_message(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save the latest checkpoint for resume purposes.\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "\n",
    "        # Monitor GPU memory usage if available.\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "            log_message(f\"GPU Memory - Allocated: {allocated:.1f} MB, Reserved: {reserved:.1f} MB\")\n",
    "except Exception as e:\n",
    "    log_message(\"An error occurred during training:\")\n",
    "    log_message(str(e))\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_elapsed = total_end_time - total_start_time\n",
    "hours, rem = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "log_message(f\"Total training runtime: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "\n",
    "# =============================\n",
    "# Optional L-BFGS Refinement (Disabled)\n",
    "# =============================\n",
    "# def closure():\n",
    "#     optimizer_lbfgs.zero_grad()\n",
    "#     loss, _, _ = combined_loss(model, x_batch, y_batch)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "#\n",
    "# optimizer_lbfgs = optim.LBFGS(model.parameters(), lr=1e-2, max_iter=500)\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.unsqueeze(1).to(device)\n",
    "#         optimizer_lbfgs.step(closure)\n",
    "#     log_message(f\"L-BFGS refinement epoch {epoch+1} complete.\")\n",
    "\n",
    "# =============================\n",
    "# Final Model Saving and Logging Close\n",
    "# =============================\n",
    "final_model_path = os.path.join(checkpoint_dir, \"pinn_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "log_message(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc35961-e6a0-4701-ace6-f3bffc2c561d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235d764-9142-4f33-bf39-7980a9c4b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "I have 2 files: /media/data-ssd/PINN/DATA/model data after proccessing/full POC model/3 final files according to 'spatial PINN Hebrew'/merged/temp_for_testing_code_dec_jan_feb/Merged_North_Dead_Sea_2012.csv and Merged_North_Dead_Sea_2013.csv.  I need you to write me a code to execute this task:  We aim to develop a Physics-Informed Neural Network (PINN) using PyTorch to predict discharge (rg_qms) at hydrometric stations based on rainfall, spatial terrain data, and temporal evolution. The model will be trained on historical data from 2012 year, tested on 2013, and run efficiently on an NVIDIA RTX 6000 GPU using batch processing and real-time monitoring. here is a description of the variables: Time Variables:\n",
    "\n",
    "time (Datetime) â€“ Timestamp of the observation.\n",
    "Spatial Features:\n",
    "\n",
    "longitude, latitude (Float) â€“ Spatial coordinates of each observation.\n",
    "elevation (Float) â€“ Elevation of each grid point.\n",
    "slope_calculated (Float) â€“ Terrain slope, useful for estimating flow direction.\n",
    "Hydrometeorological Inputs:\n",
    "\n",
    "rainrate (Float) â€“ Rainfall intensity at each location.\n",
    "rg_qms (Float) â€“ Discharge at specific hydrometric stations (our target variable).\n",
    "Data Gaps:\n",
    "\n",
    "rg_qms is only available for three specific station locations per time step.\n",
    "Other locations contain NaN values.    Task for the PINN Model:\n",
    "The PINN will be trained to:\n",
    "\n",
    "Learn the relationship between past rainfall and discharge using historical years as training data.\n",
    "Enforce physics constraints using governing equations - the kinematic wave equation.\n",
    "Predict rg_qms at future timestamps based on rainfall inputs and spatial information.\n",
    "Generalize across different years, testing model robustness.\n",
    "Model Architecture & Training Strategy (PyTorch PINN)\n",
    "1ï¸âƒ£ Input Features:\n",
    "Time-dependent inputs: rainrate, time, historical discharge values (if available).\n",
    "Spatial inputs: longitude, latitude, elevation, slope_calculated.\n",
    "2ï¸âƒ£ PINN Constraints (Loss Functions)\n",
    "Data Loss (L_data): Mean Squared Error (MSE) on discharge predictions.\n",
    "Physics Residual Loss (L_physics):\n",
    "Continuity Equation: Ensures smooth discharge variations over space and time.\n",
    "Momentum Conservation: Captures flow behavior based on terrain slope.\n",
    "Final Loss Function:\n",
    "ð¿\n",
    "=\n",
    "ð¿\n",
    "data\n",
    "+\n",
    "ðœ†\n",
    "ð¿\n",
    "physics\n",
    "L=L \n",
    "data\n",
    "â€‹\n",
    " +Î»L \n",
    "physics\n",
    "â€‹\n",
    " \n",
    "where Î» is a weighting parameter to balance data and physics loss.\n",
    "3ï¸âƒ£ Model Architecture (PyTorch)\n",
    "Fully Connected Neural Network (FCNN)\n",
    "Layers: 4-6 layers with Sine Activation (SinAct) for handling physics constraints.\n",
    "Neurons: 128-256 per layer.\n",
    "Batch Normalization: Improves training stability.\n",
    "Dropout Layers: Reduces overfitting.\n",
    "4ï¸âƒ£ Training & GPU Optimization\n",
    "Hardware: NVIDIA RTX 6000 GPU with CUDA acceleration.\n",
    "Batch Processing: Uses mini-batches of 256 or 512 samples for efficient memory usage.\n",
    "Optimization Strategy:\n",
    "Adam optimizer for fast convergence.\n",
    "L-BFGS optimizer for final refinement.\n",
    "Gradient Clipping: Prevents instability in training.\n",
    "Training & Validation Plan\n",
    "Train on: 2012\n",
    "Test on: 2013\n",
    "Evaluation Metrics:\n",
    "Root Mean Squared Error (RMSE).\n",
    "Nash-Sutcliffe Efficiency (NSE).\n",
    "RÂ² Score to measure prediction accuracy.\n",
    "Tracking Model Progress\n",
    "âœ… Live Loss Monitoring: Logs loss function values (data loss + physics loss) during training.\n",
    "âœ… Batch-wise Validation: Computes validation loss every N batches for real-time progress tracking.\n",
    "âœ… TensorBoard Integration: If needed, to visualize training curves.\n",
    "âœ… Real-Time Model Checkpoints: Saves the best model based on validation loss.\n",
    "\n",
    "Deliverables\n",
    "A fully trained PINN model capable of predicting discharge (rg_qms).\n",
    "Validation plots comparing model predictions vs. observed discharge.\n",
    "Physics-based insights into water flow behavior.\n",
    "Generalization analysis across different years.\n",
    "Final Request\n",
    "Implement this using PyTorch + CUDA, utilizing batch-based training on the RTX 6000 GPU. Ensure real-time progress tracking and visualization of predictions vs. observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d84c6-b987-42d7-a9f9-f6b422d528bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf134277-090b-45f2-9f67-80661e0ff7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26628c9d-a2b2-46b7-ab41-abd423759bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c03c0f-d7d5-4510-8eb5-9f27eb56d689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a5a88-3728-40a1-bc43-6fb868db85a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168d867-e7f7-456b-b7ca-3b25d2db9121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd71791-e35e-44e9-a95d-4eea24fcb816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
